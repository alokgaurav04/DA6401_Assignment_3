{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6xB0KDuy95e"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from collections import Counter\n",
        "from colour import Color\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVMyzAmMzE1a",
        "outputId": "57936cfd-d029-4ca9-86f2-c63de10cff96"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbXxZXZjpEq"
      },
      "source": [
        "# Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_zHEatTLsT"
      },
      "source": [
        "def get_data_files(language):\n",
        "    ## REPLACE THIS PATH UPTO dakshina_dataset_v1.0 with your own dataset path ##\n",
        "    template = \"/content/drive/MyDrive/Assignment_3/hi.translit.sampled.{}.tsv\"\n",
        "\n",
        "    train_tsv = template.format(\"train\")\n",
        "    val_tsv = template.format(\"dev\")\n",
        "    test_tsv = template.format(\"test\")\n",
        "\n",
        "    return train_tsv, val_tsv, test_tsv\n",
        "\n",
        "## Utility functions for preprocessing data ##\n",
        "\n",
        "def add_start_end_tokens(df, cols, sos=\"\\t\", eos=\"\\n\"):\n",
        "    \"\"\" Adds EOS and SOS tokens to data\n",
        "    \"\"\"\n",
        "    def add_tokens(s):\n",
        "        return sos + str(s) + eos\n",
        "\n",
        "    for col in cols:\n",
        "        df[col] = df[col].apply(add_tokens)\n",
        "\n",
        "def tokenize(lang, tokenizer=None):\n",
        "    \"\"\" Uses tf.keras tokenizer to tokenize the data/words into characters\n",
        "    \"\"\"\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(char_level=True)\n",
        "        tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "\n",
        "    else:\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "\n",
        "    return lang_tensor, tokenizer\n",
        "\n",
        "def preprocess_data(fpath, input_lang_tokenizer=None, targ_lang_tokenizer=None):\n",
        "    \"\"\" Reads, tokenizes and adds SOS/EOS tokens to data based on above functions\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(fpath, sep=\"\\t\", header=None)\n",
        "\n",
        "    # adding start and end tokens to know when to stop predicting\n",
        "    add_start_end_tokens(df, [0,1])\n",
        "\n",
        "    input_lang_tensor, input_tokenizer = tokenize(df[1].astype(str).tolist(),\n",
        "                                                    tokenizer=input_lang_tokenizer)\n",
        "\n",
        "    targ_lang_tensor, targ_tokenizer = tokenize(df[0].astype(str).tolist(),\n",
        "                                                    tokenizer=targ_lang_tokenizer)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_lang_tensor, targ_lang_tensor))\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "\n",
        "    return dataset, input_tokenizer, targ_tokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjGJ1sFE-eon"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLLikr1QN4kE"
      },
      "source": [
        "## Utility functions ##\n",
        "def get_layer(name, units, dropout, return_state=False, return_sequences=False):\n",
        "\n",
        "    if name==\"rnn\":\n",
        "        return layers.SimpleRNN(units=units, dropout=dropout,\n",
        "                                return_state=return_state,\n",
        "                                return_sequences=return_sequences)\n",
        "\n",
        "    if name==\"gru\":\n",
        "        return layers.GRU(units=units, dropout=dropout,\n",
        "                          return_state=return_state,\n",
        "                          return_sequences=return_sequences)\n",
        "\n",
        "    if name==\"lstm\":\n",
        "        return layers.LSTM(units=units, dropout=dropout,\n",
        "                           return_state=return_state,\n",
        "                           return_sequences=return_sequences)\n",
        "\n",
        "class RNNAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(RNNAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, enc_state, enc_out):\n",
        "\n",
        "    enc_state = tf.concat(enc_state, 1)\n",
        "    enc_state = tf.expand_dims(enc_state, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(enc_state) + self.W2(enc_out)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * enc_out\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, layer_type, n_layers, units, encoder_vocab_size, embedding_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer_type = layer_type\n",
        "        self.n_layers = n_layers\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(encoder_vocab_size, embedding_dim)\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        outputs=x\n",
        "\n",
        "        if (self.layer_type=='lstm'):\n",
        "            outputs, h, c = self.rnn_layers[0](outputs, initial_state=hidden)\n",
        "            state = [h, c]\n",
        "        else:\n",
        "            outputs, h = self.rnn_layers[0](outputs, initial_state=hidden)\n",
        "            state = [h]\n",
        "\n",
        "        # Remaining LSTM layers just take outputs from previous layer\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            if (self.layer_type=='lstm'):\n",
        "               outputs, h, c = layer(outputs)\n",
        "               state = [h, c]  # update state if needed\n",
        "            else:\n",
        "               outputs, h = layer(outputs)\n",
        "               state = [h]\n",
        "\n",
        "        return outputs, state\n",
        "\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "\n",
        "        if self.layer_type != \"lstm\":\n",
        "            return [tf.zeros((batch_size, self.units))]\n",
        "        else:\n",
        "            return [tf.zeros((batch_size, self.units))]*2\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, layer_type, n_layers, units, decoder_vocab_size, embedding_dim, dropout, attention=False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.layer_type = layer_type\n",
        "        self.n_layers = n_layers\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.embedding_layer = layers.Embedding(input_dim=decoder_vocab_size,\n",
        "                                                output_dim=embedding_dim)\n",
        "\n",
        "        self.dense = layers.Dense(decoder_vocab_size, activation=\"softmax\")\n",
        "        self.flatten = layers.Flatten()\n",
        "        if self.attention:\n",
        "            self.attention_layer = RNNAttention(self.units)\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden, enc_out=None):\n",
        "\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        if self.attention:\n",
        "            context_vector, attention_weights = self.attention_layer(hidden, enc_out)\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)\n",
        "        else:\n",
        "            attention_weights = None\n",
        "\n",
        "        # First LSTM layer gets the initial hidden state\n",
        "        if (self.layer_type=='lstm'):\n",
        "            x, h, c = self.rnn_layers[0](x, initial_state=hidden)\n",
        "            state = [h, c]\n",
        "        else:\n",
        "            x, h = self.rnn_layers[0](x, initial_state=hidden)\n",
        "            state = [h]\n",
        "\n",
        "        # Subsequent layers use only the output of the previous layer\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            if (self.layer_type=='lstm'):\n",
        "                x, h, c = layer(x)\n",
        "                state = [h, c]  # Update state if you want final output from last LSTM\n",
        "            else:\n",
        "                x, h = layer(x)\n",
        "                state = [h]\n",
        "\n",
        "        output = self.dense(self.flatten(x))\n",
        "\n",
        "        return output, state, attention_weights\n",
        "\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []\n",
        "\n",
        "        for i in range(self.n_layers - 1):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "\n",
        "        self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                            return_sequences=False,\n",
        "                                            return_state=True))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lks31eDH0KKc"
      },
      "source": [
        "class BeamSearch():\n",
        "    def __init__(self, model, k):\n",
        "        self.k = k\n",
        "        self.model = model\n",
        "        self.acc = tf.keras.metrics.Accuracy()\n",
        "\n",
        "    def sample_beam_search(self, probs):\n",
        "\n",
        "        m, n = probs.shape\n",
        "        output_sequences = [[[], 0.0]]\n",
        "\n",
        "        for row in probs:\n",
        "            beams = []\n",
        "\n",
        "            for tup in output_sequences:\n",
        "                seq, score = tup\n",
        "                for j in range(n):\n",
        "                    new_beam = [seq + [j], score - tf.math.log(row[j])]\n",
        "                    beams.append(new_beam)\n",
        "\n",
        "            output_sequences = sorted(beams, key=lambda x: x[1])[:self.k]\n",
        "\n",
        "        tensors, scores = list(zip(*output_sequences))\n",
        "        tensors = list(map(lambda x: tf.expand_dims(tf.constant(x),0), tensors))\n",
        "\n",
        "        return tf.concat(tensors, 0), scores\n",
        "\n",
        "    def beam_accuracy(self, input, target):\n",
        "        accs = []\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.acc.reset_states()\n",
        "            self.acc.update_state(target, input[i, :])\n",
        "            accs.append(self.acc.result())\n",
        "\n",
        "        return max(accs)\n",
        "\n",
        "    def step(self, input, target, enc_state):\n",
        "\n",
        "        batch_acc = 0\n",
        "        sequences = []\n",
        "\n",
        "        enc_out, enc_state = self.model.encoder(input, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.model.targ_tokenizer.word_index[\"\\t\"]]*self.model.batch_size ,1)\n",
        "\n",
        "        for t in range(1, target.shape[1]):\n",
        "\n",
        "            preds, dec_state, _ = self.model.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "            sequences.append(preds)\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "        sequences = tf.concat(list(map(lambda x: tf.expand_dims(x, 1), sequences)), axis=1)\n",
        "\n",
        "        for i in range(target.shape[0]):\n",
        "\n",
        "            possibilities, scores = self.sample_beam_search(sequences[i, :, :])\n",
        "            batch_acc += self.beam_accuracy(possibilities, target[i, 1:])\n",
        "\n",
        "        batch_acc = batch_acc / target.shape[0]\n",
        "\n",
        "        return 0, batch_acc\n",
        "\n",
        "    def evaluate(self, test_dataset, batch_size=None, upto=5, use_wandb=False):\n",
        "\n",
        "        if batch_size is not None:\n",
        "            self.model.batch_size = batch_size\n",
        "            test_dataset = test_dataset.batch(batch_size)\n",
        "        else:\n",
        "            self.model.batch_size = 1\n",
        "\n",
        "        test_acc = 0\n",
        "        enc_state = self.model.encoder.initialize_hidden_state(self.model.batch_size)\n",
        "\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(upto)):\n",
        "\n",
        "           _, acc = self.step(input, target, enc_state)\n",
        "           test_acc += acc\n",
        "\n",
        "        if use_wandb:\n",
        "            wandb.log({\"test acc (beam search)\": test_acc / upto})\n",
        "\n",
        "        print(f\"Test Accuracy on {upto*batch_size} samples: {test_acc / upto:.4f}\\n\")\n",
        "\n",
        "    def translate(self, word):\n",
        "\n",
        "        word = \"\\t\" + word + \"\\n\"\n",
        "        sequences = []\n",
        "        result = []\n",
        "\n",
        "        inputs = self.model.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                               maxlen=self.model.max_input_len,\n",
        "                                                               padding=\"post\")\n",
        "\n",
        "\n",
        "        enc_state = self.model.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.model.encoder(inputs, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.model.targ_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, self.model.max_target_len):\n",
        "\n",
        "            preds, dec_state, _ = self.model.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "            sequences.append(preds)\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "        sequences = tf.concat(list(map(lambda x: tf.expand_dims(x, 1), sequences)), axis=1)\n",
        "\n",
        "        possibilities, scores = self.sample_beam_search(tf.squeeze(sequences, 0))\n",
        "        output_words = self.model.targ_tokenizer.sequences_to_texts(possibilities.numpy())\n",
        "\n",
        "        def post_process(word):\n",
        "            word = word.split(\" \")[:-1]\n",
        "            return \"\".join([x for x in word])\n",
        "\n",
        "        output_words = list(map(post_process, output_words))\n",
        "\n",
        "        return output_words, scores"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwR1miyeRrIG"
      },
      "source": [
        "class Seq2SeqModel():\n",
        "    def __init__(self, embedding_dim, encoder_layers, decoder_layers, layer_type, units, dropout, attention=False):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.decoder_layers = decoder_layers\n",
        "        self.layer_type = layer_type\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.stats = []\n",
        "        self.batch_size = 128\n",
        "        self.use_beam_search = False\n",
        "\n",
        "    def build(self, loss, optimizer, metric):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.metric = metric\n",
        "\n",
        "    def set_vocabulary(self, input_tokenizer, targ_tokenizer):\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "\n",
        "        encoder_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
        "        decoder_vocab_size = len(self.targ_tokenizer.word_index) + 1\n",
        "\n",
        "        self.encoder = Encoder(self.layer_type, self.encoder_layers, self.units, encoder_vocab_size,\n",
        "                               self.embedding_dim, self.dropout)\n",
        "\n",
        "        self.decoder = Decoder(self.layer_type, self.decoder_layers, self.units, decoder_vocab_size,\n",
        "                               self.embedding_dim,  self.dropout, self.attention)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, input, target, enc_state):\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "            dec_state = enc_state\n",
        "            dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "            if random.random() < self.teacher_forcing_ratio:\n",
        "\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "\n",
        "                    dec_input = tf.expand_dims(target[:,t], 1)\n",
        "\n",
        "            else:\n",
        "\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "\n",
        "                    preds = tf.argmax(preds, 1)\n",
        "                    dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "\n",
        "            batch_loss = loss / target.shape[1]\n",
        "\n",
        "            variables = self.encoder.variables + self.decoder.variables\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "\n",
        "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "    @tf.function\n",
        "    def validation_step(self, input, target, enc_state):\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "        for t in range(1, target.shape[1]):\n",
        "\n",
        "            preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "            loss += self.loss(target[:,t], preds)\n",
        "            self.metric.update_state(target[:,t], preds)\n",
        "\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "        batch_loss = loss / target.shape[1]\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "\n",
        "    def fit(self, dataset, val_dataset, batch_size=128, epochs=10, use_wandb=False, teacher_forcing_ratio=1.0):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "        steps_per_epoch = len(dataset) // self.batch_size\n",
        "        steps_per_epoch_val = len(val_dataset) // self.batch_size\n",
        "\n",
        "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
        "        val_dataset = val_dataset.batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "        # useful when we need to translate the sentence\n",
        "        sample_inp, sample_targ = next(iter(dataset))\n",
        "        self.max_target_len = sample_targ.shape[1]\n",
        "        self.max_input_len = sample_inp.shape[1]\n",
        "\n",
        "        template = \"\\nTrain Loss: {0:.4f} Train Accuracy: {1:.4f} Validation Loss: {2:.4f} Validation Accuracy: {3:.4f}\"\n",
        "\n",
        "        print(\"-\"*100)\n",
        "        for epoch in range(1, epochs+1):\n",
        "            print(f\"EPOCH {epoch}\\n\")\n",
        "\n",
        "            ## Training loop ##\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            self.metric.reset_state()\n",
        "\n",
        "            starting_time = time.time()\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            for batch, (input, target) in enumerate(dataset.take(steps_per_epoch)):\n",
        "                batch_loss, acc = self.train_step(input, target, enc_state)\n",
        "                total_loss += batch_loss\n",
        "                total_acc += acc\n",
        "\n",
        "\n",
        "                if batch==0 or ((batch + 1) % 100 == 0):\n",
        "                    print(f\"Batch {batch+1} Loss {batch_loss:.4f}\")\n",
        "\n",
        "            avg_acc = total_acc / steps_per_epoch\n",
        "            avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "            # Validation loop ##\n",
        "            total_val_loss = 0\n",
        "            total_val_acc = 0\n",
        "            self.metric.reset_state()\n",
        "\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            for batch, (input, target) in enumerate(val_dataset.take(steps_per_epoch_val)):\n",
        "                batch_loss, acc = self.validation_step(input, target, enc_state)\n",
        "                total_val_loss += batch_loss\n",
        "                total_val_acc += acc\n",
        "\n",
        "            avg_val_acc = total_val_acc / steps_per_epoch_val\n",
        "            avg_val_loss = total_val_loss / steps_per_epoch_val\n",
        "\n",
        "            print(template.format(avg_loss, avg_acc*100, avg_val_loss, avg_val_acc*100))\n",
        "\n",
        "            time_taken = time.time() - starting_time\n",
        "            self.stats.append({\"epoch\": epoch,\n",
        "                            \"train loss\": avg_loss,\n",
        "                            \"val loss\": avg_val_loss,\n",
        "                            \"train acc\": avg_acc*100,\n",
        "                            \"val acc\": avg_val_acc*100,\n",
        "                            \"training time\": time_taken})\n",
        "\n",
        "            if use_wandb:\n",
        "                wandb.log(self.stats[-1])\n",
        "\n",
        "            print(f\"\\nTime taken for the epoch {time_taken:.4f}\")\n",
        "\n",
        "        print(\"\\nModel trained !!\")\n",
        "\n",
        "    def evaluate(self, test_dataset, batch_size=None):\n",
        "\n",
        "        if batch_size is not None:\n",
        "            self.batch_size = batch_size\n",
        "\n",
        "        steps_per_epoch_test = len(test_dataset) // batch_size\n",
        "        test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "        total_test_loss = 0\n",
        "        total_test_acc = 0\n",
        "        self.metric.reset_state()\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(steps_per_epoch_test)):\n",
        "            batch_loss, acc = self.validation_step(input, target, enc_state)\n",
        "            total_test_loss += batch_loss\n",
        "            total_test_acc += acc\n",
        "\n",
        "        avg_test_acc = total_test_acc / steps_per_epoch_test\n",
        "        avg_test_loss = total_test_loss / steps_per_epoch_test\n",
        "\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f} Test Accuracy: {avg_test_acc:.4f}\")\n",
        "\n",
        "        return avg_test_loss, avg_test_acc\n",
        "\n",
        "\n",
        "    def translate(self, word, get_heatmap=False):\n",
        "\n",
        "        word = \"\\t\" + word + \"\\n\"\n",
        "\n",
        "        inputs = self.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                               maxlen=self.max_input_len,\n",
        "                                                               padding=\"post\")\n",
        "\n",
        "        result = \"\"\n",
        "        att_wts = []\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.encoder(inputs, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, self.max_target_len):\n",
        "\n",
        "            preds, dec_state, attention_weights = self.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "            if get_heatmap:\n",
        "                att_wts.append(attention_weights)\n",
        "\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            next_char = self.targ_tokenizer.index_word[preds.numpy().item()]\n",
        "            result += next_char\n",
        "\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "            if next_char == \"\\n\":\n",
        "                return result[:-1], att_wts[:-1]\n",
        "\n",
        "        return result[:-1], att_wts[:-1]\n",
        "\n",
        "    def plot_attention_heatmap(self, word, ax, font_path=\"/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf\"):\n",
        "\n",
        "        translated_word, attn_wts = self.translate(word, get_heatmap=True)\n",
        "        attn_heatmap = tf.squeeze(tf.concat(attn_wts, 0), -1).numpy()\n",
        "\n",
        "        input_word_len = len(word)\n",
        "        output_word_len = len(translated_word)\n",
        "\n",
        "        ax.imshow(attn_heatmap[:, :input_word_len])\n",
        "\n",
        "        font_prop = FontProperties(fname=font_path, size=18)\n",
        "\n",
        "        ax.set_xticks(np.arange(input_word_len))\n",
        "        ax.set_yticks(np.arange(output_word_len))\n",
        "\n",
        "        ax.set_xticklabels(list(word))\n",
        "        ax.set_yticklabels(list(translated_word), fontproperties=font_prop)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPPCBrHukCMZ"
      },
      "source": [
        "def randomly_evaluate(model, test_file=get_data_files(\"hi\")[2], n=10):\n",
        "\n",
        "    df = pd.read_csv(test_file, sep=\"\\t\", header=None)\n",
        "    df = df.sample(n=n).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Randomly evaluating the model on {n} words\\n\")\n",
        "\n",
        "    for i in range(n):\n",
        "        word = str(df[1][i])\n",
        "\n",
        "        print(f\"Input word: {word}\")\n",
        "        print(f\"Actual translation: {str(df[0][i])}\")\n",
        "        print(f\"Model translation: {model.translate(word)[0]}\\n\")\n",
        "\n",
        "\n",
        "def test_on_dataset(language, embedding_dim, encoder_layers, decoder_layers, layer_type, units, dropout, attention, teacher_forcing_ratio=1.0, filename=None):\n",
        "\n",
        "    TRAIN_TSV, VAL_TSV, TEST_TSV = get_data_files(language)\n",
        "\n",
        "    model = Seq2SeqModel(embedding_dim,\n",
        "                         encoder_layers,\n",
        "                         decoder_layers,\n",
        "                         layer_type,\n",
        "                         units,\n",
        "                         dropout,\n",
        "                         attention)\n",
        "\n",
        "    dataset, input_tokenizer, targ_tokenizer = preprocess_data(TRAIN_TSV)\n",
        "    val_dataset, _, _ = preprocess_data(VAL_TSV, input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "\n",
        "    model.fit(dataset, val_dataset, epochs=30, use_wandb=False, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "    ## Character level accuracy ##\n",
        "    test_dataset, _, _ = preprocess_data(TEST_TSV, model.input_tokenizer, model.targ_tokenizer)\n",
        "    test_loss, test_acc = model.evaluate(test_dataset, batch_size=100)\n",
        "\n",
        "    ##  Word level accuracy ##\n",
        "    test_tsv = pd.read_csv(TEST_TSV, sep=\"\\t\", header=None)\n",
        "    inputs = test_tsv[1].astype(str).tolist()\n",
        "    targets = test_tsv[0].astype(str).tolist()\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for word in inputs:\n",
        "        outputs.append(model.translate(word)[0])\n",
        "\n",
        "    if filename is not None:\n",
        "        df = pd.DataFrame()\n",
        "        df[\"inputs\"] = inputs\n",
        "        df[\"targets\"] = targets\n",
        "        df[\"outputs\"] = outputs\n",
        "        df.to_csv(filename)\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvVSaWfGYvvn"
      },
      "source": [
        "# Visualizing Model Connectivity (Q6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeWT7Yno7LXd"
      },
      "source": [
        "# Tools for getting model connectivity between input and output characters\n",
        "\n",
        "def get_lstm_output(decoder, x, hidden, enc_out=None):\n",
        "    x = decoder.embedding_layer(x)\n",
        "\n",
        "    if decoder.attention:\n",
        "        context_vector, attention_weights = decoder.attention_layer(hidden, enc_out)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)\n",
        "    else:\n",
        "        attention_weights = None\n",
        "\n",
        "    # First layer with initial state\n",
        "    x, h, c = decoder.rnn_layers[0](x, initial_state=hidden)\n",
        "    state = [h, c]\n",
        "\n",
        "    # Remaining layers\n",
        "    for layer in decoder.rnn_layers[1:]:\n",
        "        x, h, c = layer(x)\n",
        "        state = [h, c]  # keep the last layer's state\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "def get_output_from_embedding(encoder, x, hidden):\n",
        "    x, h, c = encoder.rnn_layers[0](x, initial_state=hidden)\n",
        "    state = [h, c]\n",
        "\n",
        "    for layer in encoder.rnn_layers[1:]:\n",
        "        x, h, c = layer(x)\n",
        "        state = [h, c]\n",
        "\n",
        "    return x, state\n",
        "\n",
        "\n",
        "def get_connectivity(model, word):\n",
        "\n",
        "    word = \"\\t\" + word + \"\\n\"\n",
        "\n",
        "    inputs = model.input_tokenizer.texts_to_sequences([word])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                            maxlen=model.max_input_len,\n",
        "                                                            padding=\"post\")\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    gradient_list = []\n",
        "\n",
        "    enc_state = model.encoder.initialize_hidden_state(1)\n",
        "    embedded_in = model.encoder.embedding(inputs)\n",
        "\n",
        "\n",
        "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "        tape.watch(embedded_in)\n",
        "\n",
        "        enc_out, enc_state = get_output_from_embedding(model.encoder, embedded_in, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([model.targ_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, model.max_target_len):\n",
        "\n",
        "            lstm_out, dec_state, _ = get_lstm_output(model.decoder, dec_input, dec_state, enc_out)\n",
        "\n",
        "            preds = model.decoder.dense(model.decoder.flatten(lstm_out))\n",
        "            gradient_list.append(tape.gradient(lstm_out, embedded_in)[0])\n",
        "\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            next_char = model.targ_tokenizer.index_word[preds.numpy().item()]\n",
        "            result += next_char\n",
        "\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "            if next_char == \"\\n\":\n",
        "                return result[:-1], gradient_list[:-1]\n",
        "\n",
        "        return result[:-1], gradient_list[:-1]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22kfP4uvp63B"
      },
      "source": [
        "# Imports for visualising the model connectivity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "    if s == ' ':\n",
        "      return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "    else:\n",
        "      return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\n",
        "# print html\n",
        "def print_color(t):\n",
        "\t  display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "def get_clr(value):\n",
        "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
        "      '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "      '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "      '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "    value = int(value * 19)\n",
        "    if value == 19:\n",
        "        value -= 1\n",
        "    return colors[value]\n",
        "\n",
        "# sigmoid function\n",
        "def sigmoid(x):\n",
        "    z = 1/(1 + np.exp(-x))\n",
        "    return z\n",
        "\n",
        "def softmax(x):\n",
        "    v = np.exp(x)\n",
        "    v = v / np.sum(v)\n",
        "    return v\n",
        "\n",
        "def get_gradient_norms(grad_list, word, activation=\"sigmoid\"):\n",
        "    grad_norms = []\n",
        "    for grad_tensor in grad_list:\n",
        "        grad_mags = tf.norm(grad_tensor, axis=1)\n",
        "        grad_mags = grad_mags[:len(word)]\n",
        "        if activation == \"softmax\":\n",
        "            grad_mags_scaled = softmax(grad_mags)\n",
        "        elif activation == \"scaler\":\n",
        "            scaler = MinMaxScaler()\n",
        "            grad_mags = tf.reshape(grad_mags, (-1,1))\n",
        "            grad_mags_scaled = scaler.fit_transform(grad_mags)\n",
        "        else:\n",
        "            grad_mags_scaled = sigmoid(grad_mags)\n",
        "        grad_norms.append(grad_mags_scaled)\n",
        "    return grad_norms\n",
        "\n",
        "def visualize(grad_norms, word, translated_word):\n",
        "    print(\"Original Word:\", word)\n",
        "    print(\"Transliterated Word:\", translated_word)\n",
        "    for i in range(len(translated_word)):\n",
        "        print(\"Connectivity Visualization for\", translated_word[i],\":\")\n",
        "        text_colours = []\n",
        "        for j in range(len(grad_norms[i])):\n",
        "            text = (word[j], get_clr(grad_norms[i][j]))\n",
        "            text_colours.append(text)\n",
        "        print_color(text_colours)\n",
        "\n",
        "def visualise_connectivity(model, word, activation=\"sigmoid\"):\n",
        "    translated_word, grad_list = get_connectivity(model, word)\n",
        "    grad_norms = get_gradient_norms(grad_list, word, activation)\n",
        "    visualize(grad_norms, word, translated_word)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzkdsWRlZzTf"
      },
      "source": [
        "model = test_on_dataset(language=\"hi\",\n",
        "                        embedding_dim=256,\n",
        "                        encoder_layers=3,\n",
        "                        decoder_layers=3,\n",
        "                        layer_type=\"lstm\",\n",
        "                        units=256,\n",
        "                        dropout=0.2,\n",
        "                        attention=True,\n",
        "                        filename=\"English_To_Hindi_with_Attention.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgY9jxQo2I1Q",
        "outputId": "fabd3a2c-e9d3-4043-d99d-4f692c34b412"
      },
      "source": [
        "def get_test_words(n):\n",
        "    test_df = pd.read_csv(get_data_files(\"hi\")[2])\n",
        "    test_sample = test_df.sample(n)\n",
        "    test_sample.reset_index(inplace=True, drop=True)\n",
        "    test_words = []\n",
        "    for i in test_sample.index:\n",
        "        entry = test_sample[\"अंक\\tank\\t5\"].loc[i]\n",
        "        parts = entry.split(\"\\t\")\n",
        "        word = parts[1]\n",
        "        test_words.append(word)\n",
        "    return test_words\n",
        "\n",
        "test_words = get_test_words(5)\n",
        "print(test_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hisham', 'vadiyon', 'robertsan', 'bheed', 'denewaali']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pgDA-bWL1ueE",
        "outputId": "de2820fd-5e2b-45d9-e2f6-fd6647c9a999"
      },
      "source": [
        "for word in test_words:\n",
        "    visualise_connectivity(model, word, activation=\"scaler\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LSTM with: <class 'tensorflow.python.framework.ops.EagerTensor'> (1, 22, 256)\n",
            "Hidden is: <class 'list'> [TensorShape([1, 256]), TensorShape([1, 256])]\n",
            "Original Word: hisham\n",
            "Transliterated Word: हिशम\n",
            "Connectivity Visualization for ह :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-abcb4f2a6bda>:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  value = int(value * 19)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#f42e2e>h </text><text style=color:#000;background-color:#95cae5>i </text><text style=color:#000;background-color:#baddee>s </text><text style=color:#000;background-color:#a1d0e8#b2d9ec>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#89c4e2>m </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ि :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#f42e2e>h </text><text style=color:#000;background-color:#f9e8e8>a </text><text style=color:#000;background-color:#85c2e1>m </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for श :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>m </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for म :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>m </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LSTM with: <class 'tensorflow.python.framework.ops.EagerTensor'> (1, 22, 256)\n",
            "Hidden is: <class 'list'> [TensorShape([1, 256]), TensorShape([1, 256])]\n",
            "Original Word: vadiyon\n",
            "Transliterated Word: वाडियों\n",
            "Connectivity Visualization for व :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-abcb4f2a6bda>:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  value = int(value * 19)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#eff7fb>v </text><text style=color:#000;background-color:#baddee>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#f45f5f>i </text><text style=color:#000;background-color:#f8a8a8>y </text><text style=color:#000;background-color:#f42e2e>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ा :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>v </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#f9e8e8>i </text><text style=color:#000;background-color:#f42e2e>y </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ड :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>v </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#f42e2e>y </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ि :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>v </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for य :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>v </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ो :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>v </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ं :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>v </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>i </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LSTM with: <class 'tensorflow.python.framework.ops.EagerTensor'> (1, 22, 256)\n",
            "Hidden is: <class 'list'> [TensorShape([1, 256]), TensorShape([1, 256])]\n",
            "Original Word: robertsan\n",
            "Transliterated Word: रोबर्ट्सन\n",
            "Connectivity Visualization for र :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-abcb4f2a6bda>:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  value = int(value * 19)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#f42e2e>r </text><text style=color:#000;background-color:#89c4e2>o </text><text style=color:#000;background-color:#89c4e2>b </text><text style=color:#000;background-color:#95cae5>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#95cae5>s </text><text style=color:#000;background-color:#a1d0e8#b2d9ec>a </text><text style=color:#000;background-color:#99cce6>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ो :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#f42e2e>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ब :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for र :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ् :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ट :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ् :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for स :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for न :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>o </text><text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>t </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>n </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LSTM with: <class 'tensorflow.python.framework.ops.EagerTensor'> (1, 22, 256)\n",
            "Hidden is: <class 'list'> [TensorShape([1, 256]), TensorShape([1, 256])]\n",
            "Original Word: bheed\n",
            "Transliterated Word: भीड़\n",
            "Connectivity Visualization for भ :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-abcb4f2a6bda>:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  value = int(value * 19)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#f42e2e>b </text><text style=color:#000;background-color:#baddee>h </text><text style=color:#000;background-color:#f9e8e8>e </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#89c4e2>d </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ी :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#95cae5>e </text><text style=color:#000;background-color:#f42e2e>d </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ड :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>d </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ़ :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>b </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>d </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LSTM with: <class 'tensorflow.python.framework.ops.EagerTensor'> (1, 22, 256)\n",
            "Hidden is: <class 'list'> [TensorShape([1, 256]), TensorShape([1, 256])]\n",
            "Original Word: denewaali\n",
            "Transliterated Word: देनेवाली\n",
            "Connectivity Visualization for द :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-abcb4f2a6bda>:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  value = int(value * 19)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#f42e2e>d </text><text style=color:#000;background-color:#f9bdbd>e </text><text style=color:#000;background-color:#a1d0e8#b2d9ec>n </text><text style=color:#000;background-color:#f8a8a8>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#eff7fb>a </text><text style=color:#000;background-color:#89c4e2>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#a1d0e8#b2d9ec>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for े :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#f42e2e>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for न :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for े :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for व :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ा :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ल :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connectivity Visualization for ी :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>d </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>n </text><text style=color:#000;background-color:#85c2e1>e </text><text style=color:#000;background-color:#85c2e1>w </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>l </text><text style=color:#000;background-color:#85c2e1>i </text>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "model.plot_attention_heatmap('robertsan', ax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "BGwRzwxxmHhW",
        "outputId": "2a953e66-de07-4fa1-e050-36dd2c014426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApcAAAKTCAYAAABM/SOHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAI5dJREFUeJzt3X+U1XWd+PHXHQYuLjATYBTi8GPKXBWTsthF2XWOZuZm62rpURc2WswfHHO1IxC77BHX9QzHsiT3WK57WNFddYvqCOoxqc6YIbFysEQzv1JxhESNsBkCvQJzv398v87uhCDMvIbPHXk8zvmczp37uff98lPNefq59/OZUrVarQYAACSoK3oAAADePsQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAaeqLHuAPdXZ2xgsvvBDDhg2LUqlU9DgAAIe8arUa27ZtiyOOOCLq6vZ9brLm4vKFF16IpqamoscAAOAPbNy4MY488sh97lNzcTls2LCIiDhl+EVRXxpU8DQAbw/VSqXoEaBfKZXLRY9QU3ZVX49HXrm7q9P2pebi8o2PwutLg6K+TlwCZKiW/KVfOBAlDdJd5//7j/35yqILegAASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIIy4BAEgjLgEASCMuAQBIkxKXbW1tUSqV9nsDAODtqT7jTcaOHRtz587d6/N33313bNu2LS699NKM5QAAqFGlarVa7etFWlpaYtOmTbF+/fq33LejoyMaGxvjtBEzor5uUF+PBnBIqL5WKXoE6FdKg8tFj1BTdnW+Ht/feke0t7dHQ0PDPvf1nUsAANKkxuXjjz8eW7ZsyXxLAAD6kbS4XLVqVUyePPlNv3u5devWGDp06Ju+rlKpREdHR7cNAID+KS0up0yZEieddFIsXrw4Lrroonjqqadi+/btceutt8a6devi9NNPf9PXtba2RmNjY9fW1NSUNRIAAAdZ6gU9q1atipNOOmmPn0+dOjUefPDBGDZs2B7PVSqVqFT+54vmHR0d0dTU5IIegEQu6IED44Ke7g7kgp6UWxG9YcqUKTF9+vS46667Yvr06XHUUUfFxIkT4+yzz466ujc/SVoul6Nc9l8gAMDbQfqtiNrb2+P444+PnTt3xs9+9rMYPnz4Ab3erYgA8jlzCQfGmcvuCr0VUWNjY9xxxx3x0ksvxYwZM+Ig3EYTAIAa0Sf3uTz11FPjhhtuiGXLlsX111/fF0sAAFCDUr9z+b/NmzcvNmzYENdee22MGjUqLrvssr5aCgCAGtFncRkRceutt8ZLL70Us2bNimq1GpdffnlfLgcAQMH69M8/DhgwIJYuXRrTpk2LWbNmxZe+9KW+XA4AgIL16ZnLiIj6+vpYsmRJjBkzJk455ZS+Xg4AgAL1eVxGRJRKpWhtbT0YSwEAUKA+/VgcAIBDi7gEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgTX3RA+zN7q2vRKk0sOgxqFWlUtET1JTv/vqJokeoOWce/WdFj1BTOrdvL3oE6F/8f6ab3dWd+72vM5cAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACk6XVc3nHHHXHTTTft9fl///d/j6985Su9XQYAgH6g13H585//PK655pp48skn93jut7/9bcyaNStWrVrV22UAAOgHeh2Xn/vc52Lo0KHx13/91/Hiiy92/Xzbtm1x4YUXxs6dO2POnDm9XQYAgH6gvrdvMGbMmPjP//zPOO+88+KYY46Js846KwYNGhQPPfRQbN68Of7lX/4lPvShD2XMCgBAjet1XEZE/OVf/mWsWrUqrr322rjvvvti9+7d8Sd/8idx1113xamnnpqxBAAA/UBKXEZEfPCDH4zly5dnvR0AAP2QWxEBAJAmPS7b2tqiVCq96TZ+/Pjs5QAAqCFpH4u/YezYsTF37tyIiHjsscfi0UcfjcsuuywaGxtj+PDh2csBAFBD0uOyubk5Fi5cGBER//zP/xyPPvpozJ0711lLAIBDgO9cAgCQRlwCAJAm/WPxA1WpVKJSqXQ97ujoKHAaAAB6o/Azl62trdHY2Ni1NTU1FT0SAAA9VHhczps3L9rb27u2jRs3Fj0SAAA9VPjH4uVyOcrlctFjAACQoPAzlwAAvH2ISwAA0ohLAADSiEsAANKISwAA0ohLAADSiEsAANL0aVzOnz8/qtVqjB8/vi+XAQCgRjhzCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAECa+qIHgB6pVoueoKb8xQc+WvQINef9P3y56BFqytorP1D0CDWl7kc/KXqE2uP3KkmcuQQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACBNj+Oyra0tSqXSAW8LFixIHB8AgFpS39MXjh07NubOnfuW+912220xbNiwuOiiiyIiYurUqT1dEgCAGtfjuGxubo6FCxe+5X5Lly6NI488cr/2BQCgf+txXP6h119/Pe6///546qmn4o/+6I/ilFNOiQ9/+MNZbw8AQD+QEpdr1qyJ888/P371q191/WzSpEnxxBNPZLw9AAD9RK/j8pe//GWcfvrp0d7eHvfcc0+ceeaZsWvXrtiyZct+vb5SqUSlUul63NHR0duRAAAoSK9vRbRgwYL43e9+F6NGjYrrr78+Nm/eHCNHjoyjjz56v17f2toajY2NXVtTU1NvRwIAoCC9jsuHH344pk6dGk888UQMHTo0PvzhD8c3v/nN/X79vHnzor29vWvbuHFjb0cCAKAgvY7L3//+9zFq1KgYPXp0PPLII3HWWWfF+eefH1dffXW8/vrr8dprr+3z9eVyORoaGrptAAD0T72OyxNOOCEeffTReOWVV2Lw4MFxzz33xIIFC2LRokXxnve8J379619nzAkAQD/Q67icP39+bNmyJT72sY/FY489Fjt27IhPfepTcdxxx8WmTZsyZgQAoJ/odVyeeeaZcfvtt8e6devi5JNPjiFDhsTEiRPjueeeiwULFsSECRMy5gQAoB9Iuc/lzJkz46yzzor77rsvNm3aFEcccUR84hOfiDFjxsRdd92VsQQAAP1A2l/oede73hWXXHJJ1tsBANAP9fpjcQAAeIO4BAAgTdrH4nuzfv36vl4CAIAa4cwlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnqix4A6L3dL71c9Ag158kpg4seoaZ8Z/3Xih6hppz3vlOLHqHmdO7YUfQIvE04cwkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQJqUuGxra4tSqbTfGwAAb0/1GW8yduzYmDt37l6fv/vuu2Pbtm1x6aWXZiwHAECNSonL5ubmWLhw4V6f//GPfxybNm3a5z4AAPR/vnMJAECa1Lh8/PHHY8uWLZlvCQBAP5IWl6tWrYrJkye/6Xcvt27dGkOHDn3T11Uqlejo6Oi2AQDQP6XF5ZQpU+Kkk06KxYsXx0UXXRRPPfVUbN++PW699dZYt25dnH766W/6utbW1mhsbOzampqaskYCAOAgK1Wr1WrWm61atSpOOumkPX4+derUePDBB2PYsGF7PFepVKJSqXQ97ujoiKampmiJs6O+NDBrNOAQUzd4cNEj1JRvrm8reoSact77Ti16hJrTuWNH0SNQw3ZVd0Zb3Bft7e3R0NCwz31TrhZ/w5QpU2L69Olx1113xfTp0+Ooo46KiRMnxtlnnx11dW9+krRcLke5XM4cAwCAgqTGZUTELbfcEm1tbbFixYpYtGhRDB8+PHsJAABqVPqtiBobG+OOO+6Il156KWbMmBGJn7oDAFDj+uQ+l6eeemrccMMNsWzZsrj++uv7YgkAAGpQ+sfib5g3b15s2LAhrr322hg1alRcdtllfbUUAAA1os/iMiLi1ltvjZdeeilmzZoV1Wo1Lr/88r5cDgCAgvXpn38cMGBALF26NKZNmxazZs2KL33pS325HAAABevTM5cREfX19bFkyZIYM2ZMnHLKKX29HAAABerzuIyIKJVK0draejCWAgCgQH36sTgAAIcWcQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQJr6ogcA6Audr+8seoSacv4Jf1H0CDXl2VsnFD1CzWlYWy56hJryrq8+VvQI/ZYzlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkEZcAAKQRlwAApBGXAACkqS96gEqlEpVKpetxR0dHgdMAANAbhZ+5bG1tjcbGxq6tqamp6JEAAOihwuNy3rx50d7e3rVt3Lix6JEAAOihwj8WL5fLUS6Xix4DAIAEhZ+5BADg7UNcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJCmz+Py1Vdf7eslAACoEX0alz/5yU9i0qRJcc4550R7e3tfLgUAQA3os7isVqsxbdq0GD16dDz55JNx4oknxk9/+tO+Wg4AgBrQZ3FZKpVi2bJlsWLFilizZk0cffTRMWXKlFi8eHFfLQkAQMH69GPx5ubmGDhwYAwfPjyWL18es2fPjosvvjhmzpwZr732Wl8uDQBAAQ7a1eJ1dXVx3XXXxfLly+Pb3/523HbbbQdraQAADpL6g73gxz/+8Xj22Wdj1KhRB3tpAAD6WCH3uRSWAABvT26iDgBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAEAacQkAQBpxCQBAGnEJAECa+qIHAOgTnbuLnqCm7H6lvegRaspRn1lb9Ag1Z+dpHyx6hJqy/q4PFD1CTenc8VrEJfft177OXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJCmR3HZ1tYWpVKpR1tbW1vyPwIAALWivicvGjt2bMydO/ct9/vRj34UK1eujFmzZsWwYcO6XgsAwNtTj+Kyubk5Fi5c+Jb7LViwIFauXBnz5s2LI488sidLAQDQj/jOJQAAacQlAABpevSx+P/28ssvx+zZs2PZsmXxu9/9LmEkAAD6q17F5bZt2+KUU06J5557Lj75yU/GhAkT3nS/hoaGvb5HpVKJSqXS9bijo6M3IwEAUKBexeVXvvKV+PnPfx733HNPXHDBBT16j9bW1rjuuut6MwYAADWiV9+5fOihh2LcuHE9DsuIiHnz5kV7e3vXtnHjxt6MBABAgXp15rK9vT1Gjx7dqwHK5XKUy+VevQcAALWhV2cuR48eHRs2bIhqtZo1DwAA/Viv4vIjH/lIvPjii/GDH/wgax4AAPqxXsXlZz/72WhsbIwrrrgi2tvb97rfb37zm5g/f37s3r27N8sBAFDjehWXI0eOjMWLF8ezzz4bf/7nfx7r1q3bY59169bFaaedFjfccEPcf//9vVkOAIAa1+ubqJ977rnxjW98Iz7zmc/EpEmTYurUqfH+978/du/eHevWrYuVK1fGgAED4oYbboizzz47Y2YAAGpUr+MyIuJTn/pUTJ06NW6++eZ44IEHYvHixVFXVxdNTU3xuc99Li655JI47rjjMpYCAKCGpcRlRMS73/3uWLhwYSxcuDDrLQEA6Gd69Z1LAAD438QlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnqix4AgIOg2ln0BNS4wc//rugRakq1s7HoEWpKtVra732duQQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgjbgEACCNuAQAII24BAAgTX3RA1QqlahUKl2POzo6CpwGAIDeKPzMZWtrazQ2NnZtTU1NRY8EAEAPFR6X8+bNi/b29q5t48aNRY8EAEAPFf6xeLlcjnK5XPQYAAAkKPzMJQAAbx/iEgCANOISAIA04hIAgDTiEgCANOISAIA0fR6Xr776al8vAQBAjejTuPzJT34SkyZNinPOOSfa29v7cikAAGpAn8VltVqNadOmxejRo+PJJ5+ME088MX7605/21XIAANSAPovLUqkUy5YtixUrVsSaNWvi6KOPjilTpsTixYv7akkAAArWpx+LNzc3x8CBA2P48OGxfPnymD17dlx88cUxc+bMeO211/pyaQAACnDQrhavq6uL6667LpYvXx7f/va347bbbjtYSwMAcJDUH+wFP/7xj8ezzz4bo0aNOthLAwDQxwq5z6WwBAB4e3ITdQAA0ohLAADSiEsAANKISwAA0ohLAADSiEsAANKISwAA0ohLAADSiEsAANKISwAA0ohLAADSiEsAANKISwAA0ohLAADSiEsAANKISwAA0ohLAADS1Bc9AAAHQbVa9ATUuN3/5xdFj1BTjr50cNEj1JRd1ddj437u68wlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABp9jsu29raolQq9XpraWnpw38cAACKVL+/O44dOzbmzp37ps99+ctfjubm5virv/qrPZ574IEH4umnn445c+ZERERzc3PPJgUAoOaVqtVqtbdvMnTo0PjYxz4WS5cu3eO5GTNmxH/8x3/Erl279uu9Ojo6orGxMVri7KgvDeztaAAAB6xu8OCiR6gpu6qvxw9e+0a0t7dHQ0PDPvf1nUsAANKISwAA0hxQXFYqlZg7d25s3bo1bYBKpRIdHR3dNgAA+qcDistVq1bFF7/4xZgxY0bs3LkzIiLWr18f27dvj8MPP7xHA7S2tkZjY2PX1tTU1KP3AQCgeAd8Qc+cOXPii1/8Yhx33HHR0tIS9913X2zatGmfrxkwYMBeL+ipVCpRqVS6Hnd0dERTU5MLegCAwrigp7sDuaDngONy586d8Wd/9mexevXqKJfLMXLkyDj33HNjyJAhb7r/Aw88EM8884yrxQGAfkNcdncgcbnf97l8w8CBA+Oee+6JD3zgA3HyySfH/fffH6VSaa/7v/jii/HMM88c6DIAAPRDPbpafMKECXH77bfHgw8+GDfeeGP2TAAA9FM9vhXReeedF5deemnMnz8/fvSjH2XOBABAP9Wr+1zefPPN8cd//MdxwQUXxJYtW7JmAgCgn+pVXA4ePDj+67/+K1555ZWYNm1aJPwlSQAA+rFe/4WeY489Nr761a/Gd7/73Whtbc2YCQCAfirlzz/OnDkzLrjggnj44Yf3+5ZDAAC8/RzwfS73plKpxKBBg/Z5W6L94T6XAEDR3Oeyuz69z+XelMvlrLcCAKCfSvlYHAAAIsQlAACJxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnEJQAAacQlAABpxCUAAGnqix4AAKgBpVLRE9SU9rMnFT1CTdm187WIb31jv/Z15hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDQ9isu2trYolUo92lpaWpL/EQAAqBX1PXnR2LFjY+7cufu177333huvvPJKXH755RER0dzc3JMlAQDoB3oUl83NzbFw4cL92nfNmjWxfv36/d4fAID+y3cuAQBIIy4BAEjTo4/FIyK2bt0aX/va12Lbtm373G/9+vX7fL5SqUSlUul63NHR0dORAAAoWI/j8sEHH4z58+fv177jxo3b63Otra1x3XXX9XQMAABqSI8/Fv/kJz8ZEyZM6Hp8xhlnxLZt26JarXbbTjvttH2+z7x586K9vb1r27hxY09HAgCgYD2Oy8MOOyzuvffeKJfLERExfvz4GDp06AG/T7lcjoaGhm4bAAD9U68u6Jk8eXLceeedUV9fHyNGjMiaCQCAfqrH37l8w/nnnx+TJk2KMWPGZMwDAEA/1uu4jIh43/vel/E2AAD0c+5zCQBAGnEJAEAacQkAQBpxCQBAmpQLevble9/7Xl8vAQBAjXDmEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANOISAIA04hIAgDTiEgCANPVFD/CHqtVqRETsip0R1YKHAYBDRqnoAWrKrp2vFT1CTdn9/4/HG522L6Xq/ux1EG3atCmampqKHgMAgD+wcePGOPLII/e5T83FZWdnZ7zwwgsxbNiwKJWK+7eojo6OaGpqio0bN0ZDQ0Nhc9QSx6Q7x2NPjkl3jseeHJPuHI/uHI891coxqVarsW3btjjiiCOirm7f36qsuY/F6+rq3rKID6aGhgb/A/8Djkl3jseeHJPuHI89OSbdOR7dOR57qoVj0tjYuF/7uaAHAIA04hIAgDTici/K5XJce+21US6Xix6lZjgm3Tkee3JMunM89uSYdOd4dOd47Kk/HpOau6AHAID+y5lLAADSiEsAANKISwAA0ohLAADSiEs4QC0tLXHVVVcVPQYA1CRxCcBB4V/M4NAgLvfT66+/XvQIQD/mdwhwqBCXe9HS0hJXXHFFXHXVVXH44YfHGWecUfRIhalUKnHllVfGqFGjYvDgwTF16tR4/PHHix6rULt27YorrrgiGhsb4/DDD49//Md/jEP5lrGdnZ3R2toaEyZMiMMOOyxOOOGEWLp0adFjFcrvkO5mzJgRjzzySCxatChKpVKUSqXYsGFD0WMVaunSpXH88cfHYYcdFiNHjoyPfOQjsX379qLHKsxDDz0UU6dOjXe84x0xcuTIOOuss+IXv/hF0WMVpqWlJa688sqYM2dOjBgxIt797nfHggULih5rv4jLfViyZEkMGjQoVq5cGV//+teLHqcwc+bMiW9961uxZMmSWLt2bbz3ve+NM844I7Zu3Vr0aIVZsmRJ1NfXx3//93/HokWL4stf/nL827/9W9FjFaa1tTXuvPPO+PrXvx5PP/10XH311TFt2rR45JFHih6tUH6H/I9FixbFlClT4rOf/Wxs3rw5Nm/eHE1NTUWPVZjNmzfHhRdeGH/7t38bzzzzTLS1tcW55557SP9L6vbt2+Pzn/98rFmzJr7//e9HXV1dnHPOOdHZ2Vn0aIVZsmRJDBkyJFavXh033nhj/NM//VOsWLGi6LHekr/QsxctLS3R0dERa9euLXqUQm3fvj2GDx8ed9xxR1x00UUREbFz584YP358XHXVVTF79uyCJzz4Wlpa4uWXX46nn346SqVSRER84QtfiGXLlsXPfvazgqc7+CqVSowYMSK+973vxZQpU7p+fvHFF8eOHTvi7rvvLnC64vgdsqeWlpaYNGlS3HzzzUWPUri1a9fGiSeeGBs2bIhx48YVPU5N2rJlS7zzne+MdevWxcSJE4se56BraWmJ3bt3x6OPPtr1s8mTJ8epp54aCxcuLHCyt+bM5T6ceOKJRY9QuF/84hexc+fOOPnkk7t+NnDgwJg8eXI888wzBU5WrD/90z/tCsuIiClTpsRzzz0Xu3fvLnCqYqxfvz527NgRp59+egwdOrRru/POOw/pj7Qi/A5h70444YQ47bTT4vjjj4/zzjsvbr/99njllVeKHqtQzz33XFx44YXR3NwcDQ0NMX78+IiIeP7554sdrEDvf//7uz0ePXp0vPzyywVNs//qix6glg0ZMqToEaDm/f73v4+IiAceeCDGjBnT7blyuVzESDXD7xD2ZsCAAbFixYp47LHH4uGHH45bbrkl/uEf/iFWr14dEyZMKHq8QnziE5+IcePGxe233x5HHHFEdHZ2xsSJEw/pi+EGDhzY7XGpVOoXXxNw5pJ9es973tP1nbE37Ny5Mx5//PE49thjC5ysWKtXr+72+Mc//nEcddRRMWDAgIImKs6xxx4b5XI5nn/++Xjve9/bbTuUv1PHngYNGnRInt3fm1KpFCeffHJcd9118cQTT8SgQYPiO9/5TtFjFeK3v/1tPPvsszF//vw47bTT4phjjjnkz+T2Z85csk9DhgyJyy+/PGbPnh0jRoyIsWPHxo033hg7duyImTNnFj1eYZ5//vn4/Oc/H5deemmsXbs2brnllrjpppuKHqsQw4YNi2uuuSauvvrq6OzsjKlTp0Z7e3usXLkyGhoa4tOf/nTRI1Ijxo8fH6tXr44NGzbE0KFDY8SIEVFXd2ie41i9enV8//vfj49+9KMxatSoWL16dfzmN7+JY445pujRCjF8+PAYOXJk/Ou//muMHj06nn/++fjCF75Q9Fj0kLjkLS1cuDA6Oztj+vTpsW3btvjQhz4U3/3ud2P48OFFj1aYv/mbv4lXX301Jk+eHAMGDIi/+7u/i0suuaTosQpz/fXXxzvf+c5obW2NX/7yl/GOd7wjPvjBD8bf//3fFz0aNeSaa66JT3/603HsscfGq6++Gr/61a+6vld3qGloaIgf/vCHcfPNN0dHR0eMGzcubrrppjjzzDOLHq0QdXV1ce+998aVV14ZEydOjKOPPjq++tWvRktLS9Gj0QOuFgcAIM2h+XkEAAB9QlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJBGXAIAkEZcAgCQRlwCAJDm/wKfF2nQCiYdUQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E9pUPNGndTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbad3ee-7e84-4462-f3ff-1b168e46d3e3"
      },
      "source": [
        "randomly_evaluate(model, n=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly evaluating the model on 20 words\n",
            "\n",
            "Input word: piky\n",
            "Actual translation: पिकी\n",
            "Model translation: पिकी\n",
            "\n",
            "Input word: wipro\n",
            "Actual translation: विप्रो\n",
            "Model translation: विप्रो\n",
            "\n",
            "Input word: chhupata\n",
            "Actual translation: छुपता\n",
            "Model translation: छुपता\n",
            "\n",
            "Input word: jassi\n",
            "Actual translation: जस्सी\n",
            "Model translation: जस्सी\n",
            "\n",
            "Input word: tahkhana\n",
            "Actual translation: तहखाना\n",
            "Model translation: तहखाना\n",
            "\n",
            "Input word: cube\n",
            "Actual translation: क्यूब\n",
            "Model translation: कब\n",
            "\n",
            "Input word: tel\n",
            "Actual translation: तेल\n",
            "Model translation: टेल\n",
            "\n",
            "Input word: jaydayal\n",
            "Actual translation: जयदयाल\n",
            "Model translation: जायदायल\n",
            "\n",
            "Input word: sabah\n",
            "Actual translation: साबाह\n",
            "Model translation: सबाह\n",
            "\n",
            "Input word: kihatkati\n",
            "Actual translation: खटकती\n",
            "Model translation: किहतकति\n",
            "\n",
            "Input word: bakayon\n",
            "Actual translation: बकायों\n",
            "Model translation: बकायों\n",
            "\n",
            "Input word: andrabi\n",
            "Actual translation: अंद्राबी\n",
            "Model translation: अंदराबी\n",
            "\n",
            "Input word: fagua\n",
            "Actual translation: फगुआ\n",
            "Model translation: फगुआ\n",
            "\n",
            "Input word: chowk\n",
            "Actual translation: चौक\n",
            "Model translation: चॉक\n",
            "\n",
            "Input word: wonder\n",
            "Actual translation: वंडर\n",
            "Model translation: वॉन्डर\n",
            "\n",
            "Input word: bajrangi\n",
            "Actual translation: बजरंगी\n",
            "Model translation: बाजरंगी\n",
            "\n",
            "Input word: anudit\n",
            "Actual translation: अनूदित\n",
            "Model translation: अनुदित\n",
            "\n",
            "Input word: samanjasy\n",
            "Actual translation: सामंजस्य\n",
            "Model translation: समंजास्य\n",
            "\n",
            "Input word: nilabh\n",
            "Actual translation: नीलाभ\n",
            "Model translation: निलभ\n",
            "\n",
            "Input word: raajon\n",
            "Actual translation: राजों\n",
            "Model translation: राजों\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVjQ4r9f1Xoz"
      },
      "source": [
        "# WandB Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYADdILDERar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "07003dfd-c010-42cd-af58-484cea627799"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malokgaurav04\u001b[0m (\u001b[33malokgaurav04-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_FiH40wgm0i"
      },
      "source": [
        "def train_with_wandb(language, test_beam_search=False):\n",
        "\n",
        "    config_defaults = {\"embedding_dim\": 64,\n",
        "                       \"enc_dec_layers\": 1,\n",
        "                       \"layer_type\": \"lstm\",\n",
        "                       \"units\": 128,\n",
        "                       \"dropout\": 0,\n",
        "                       \"attention\": False,\n",
        "                       \"beam_width\": 3,\n",
        "                       \"teacher_forcing_ratio\": 1.0\n",
        "                       }\n",
        "\n",
        "    wandb.init(config=config_defaults, project=\"DA6401_Assignment_3\", resume=True)\n",
        "\n",
        "    ## 1. SELECT LANGUAGE ##\n",
        "    TRAIN_TSV, VAL_TSV, TEST_TSV = get_data_files(language)\n",
        "\n",
        "    ## 2. DATA PREPROCESSING ##\n",
        "    dataset, input_tokenizer, targ_tokenizer = preprocess_data(TRAIN_TSV)\n",
        "    val_dataset, _, _ = preprocess_data(VAL_TSV, input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    ## 3. CREATING THE MODEL ##\n",
        "    model = Seq2SeqModel(embedding_dim=wandb.config.embedding_dim,\n",
        "                         encoder_layers=wandb.config.enc_dec_layers,\n",
        "                         decoder_layers=wandb.config.enc_dec_layers,\n",
        "                         layer_type=wandb.config.layer_type,\n",
        "                         units=wandb.config.units,\n",
        "                         dropout=wandb.config.dropout,\n",
        "                         attention=wandb.config.attention)\n",
        "\n",
        "    ## 4. COMPILING THE MODEL\n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "\n",
        "    ## 5. FITTING AND VALIDATING THE MODEL\n",
        "    model.fit(dataset, val_dataset, epochs=10, use_wandb=True, teacher_forcing_ratio=wandb.config.teacher_forcing_ratio)\n",
        "\n",
        "    if test_beam_search:\n",
        "        ## OPTIONAL :- Evaluate the dataset using beam search and without beam search\n",
        "        val_dataset, _, _ = preprocess_data(VAL_TSV, model.input_tokenizer, model.targ_tokenizer)\n",
        "        subset = val_dataset.take(500)\n",
        "\n",
        "        # a) Without beam search\n",
        "        _, test_acc_without = model.evaluate(subset, batch_size=100)\n",
        "        wandb.log({\"test acc\": test_acc_without})\n",
        "\n",
        "        # b) With beam search\n",
        "        beam_search = BeamSearch(model=model, k=wandb.config.beam_width)\n",
        "        beam_search.evaluate(subset, batch_size=100, use_wandb=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TIsyV5QF0M7"
      },
      "source": [
        "# Sweeps without Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byYn_WLZMQ5i"
      },
      "source": [
        "sweep_config = {\n",
        "  \"method\": \"grid\",\n",
        "  \"metric\":{\n",
        "      'name':'val acc',\n",
        "      'goal':'maximize'\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"enc_dec_layers\": {\n",
        "           \"values\": [1, 2, 3]                 # Number of Encoder and Decoder layer\n",
        "        },\n",
        "        \"units\": {\n",
        "            \"values\": [64, 128, 256]           # Dimensionality\n",
        "        },\n",
        "        \"layer_type\": {\n",
        "            \"values\": [\"rnn\", \"gru\", \"lstm\"]   # Cell Type\n",
        "        },\n",
        "        \"embedding_dim\": {\n",
        "            \"values\": [64, 128, 256]           #Embedding size\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2, 0.3]               #Dropout\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h25tYWs4T3wV",
        "outputId": "c62bbd67-f576-40d3-b94a-c497f4d3104e"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_3\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: umh6hves\n",
            "Sweep URL: https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/umh6hves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70hy-l9UAa9"
      },
      "source": [
        "wandb.agent(sweep_id, function=lambda: train_with_wandb(\"hi\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmCPXKlFz4_T"
      },
      "source": [
        "sweep_config3 = {\n",
        "  \"name\": \"Sweep 3- Assignment3\",\n",
        "  \"method\": \"grid\",\n",
        "  \"metric\":{\n",
        "      'name':'val acc',\n",
        "      'goal':'maximize'\n",
        "  },\n",
        "  \"parameters\": {\n",
        "\n",
        "        \"beam_width\": {\n",
        "            \"values\": [3, 5, 7]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKNSElH9lman",
        "outputId": "f7c71b3e-bf61-4200-e579-2243c28b9e01"
      },
      "source": [
        "sweep_id3 = wandb.sweep(sweep_config3, project=\"DA6401_Assignment_3\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: k43xn9pq\n",
            "Sweep URL: https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/k43xn9pq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys0LWC8Bl0Mc"
      },
      "source": [
        "wandb.agent(sweep_id3, function=lambda: train_with_wandb(\"hi\", test_beam_search=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kx44JYk6uXr"
      },
      "source": [
        "# Sweeps with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6ZvJVNy6mxE"
      },
      "source": [
        "sweep_config4 = {\n",
        "  \"name\": \"Sweep Attention\",\n",
        "  \"method\": \"grid\",\n",
        "  \"metric\":{\n",
        "      'name':'val acc',\n",
        "      'goal':'maximize'\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"enc_dec_layers\": {\n",
        "           \"values\": [1, 2, 3]\n",
        "        },\n",
        "        \"units\": {\n",
        "            \"values\": [128, 256]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0, 0.2]\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"values\": [True]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AntGSlwU7RDm",
        "outputId": "aa840eb9-efa2-4202-f075-28bc519b9db4"
      },
      "source": [
        "sweep_id4 = wandb.sweep(sweep_config4, project=\"DA6401_Assignment_3\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 691yfjl7\n",
            "Sweep URL: https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YZtXMBc47h2Z",
        "outputId": "fad8583a-7ac9-466d-ba90-43ec9436c3fd"
      },
      "source": [
        "wandb.agent(sweep_id4, function=lambda: train_with_wandb(\"hi\"), project=\"DA6401_Assignment_3\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xr2bqdyn with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_162414-xr2bqdyn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/xr2bqdyn' target=\"_blank\">mild-sweep-1</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/xr2bqdyn' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/xr2bqdyn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9904\n",
            "Batch 100 Loss 1.0334\n",
            "Batch 200 Loss 0.9856\n",
            "Batch 300 Loss 0.9373\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.0794 Train Accuracy: 68.2295 Validation Loss: 1.9414 Validation Accuracy: 53.9737\n",
            "\n",
            "Time taken for the epoch 37.5776\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8995\n",
            "Batch 100 Loss 0.9445\n",
            "Batch 200 Loss 0.8068\n",
            "Batch 300 Loss 0.6582\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8070 Train Accuracy: 75.2459 Validation Loss: 1.9383 Validation Accuracy: 56.8561\n",
            "\n",
            "Time taken for the epoch 14.1430\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6576\n",
            "Batch 100 Loss 0.5533\n",
            "Batch 200 Loss 0.4891\n",
            "Batch 300 Loss 0.3808\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5248 Train Accuracy: 81.6972 Validation Loss: 1.4740 Validation Accuracy: 70.4962\n",
            "\n",
            "Time taken for the epoch 14.4757\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3543\n",
            "Batch 100 Loss 0.2984\n",
            "Batch 200 Loss 0.2813\n",
            "Batch 300 Loss 0.2631\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2946 Train Accuracy: 89.9489 Validation Loss: 1.4588 Validation Accuracy: 73.9780\n",
            "\n",
            "Time taken for the epoch 13.8712\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2584\n",
            "Batch 100 Loss 0.2237\n",
            "Batch 200 Loss 0.2141\n",
            "Batch 300 Loss 0.2131\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2208 Train Accuracy: 92.6910 Validation Loss: 1.1709 Validation Accuracy: 80.3612\n",
            "\n",
            "Time taken for the epoch 14.3722\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2246\n",
            "Batch 100 Loss 0.1753\n",
            "Batch 200 Loss 0.1877\n",
            "Batch 300 Loss 0.1709\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1889 Train Accuracy: 93.6919 Validation Loss: 1.1974 Validation Accuracy: 80.5847\n",
            "\n",
            "Time taken for the epoch 14.0997\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1682\n",
            "Batch 100 Loss 0.1490\n",
            "Batch 200 Loss 0.1563\n",
            "Batch 300 Loss 0.1336\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1714 Train Accuracy: 94.2967 Validation Loss: 1.1490 Validation Accuracy: 82.1981\n",
            "\n",
            "Time taken for the epoch 14.4999\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1490\n",
            "Batch 100 Loss 0.1612\n",
            "Batch 200 Loss 0.1321\n",
            "Batch 300 Loss 0.1545\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1578 Train Accuracy: 94.8009 Validation Loss: 1.1446 Validation Accuracy: 82.0794\n",
            "\n",
            "Time taken for the epoch 14.0657\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1487\n",
            "Batch 100 Loss 0.1480\n",
            "Batch 200 Loss 0.1567\n",
            "Batch 300 Loss 0.1528\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1477 Train Accuracy: 95.0572 Validation Loss: 1.1633 Validation Accuracy: 82.4628\n",
            "\n",
            "Time taken for the epoch 14.6014\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1282\n",
            "Batch 100 Loss 0.1302\n",
            "Batch 200 Loss 0.1581\n",
            "Batch 300 Loss 0.1457\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1385 Train Accuracy: 95.3733 Validation Loss: 1.1264 Validation Accuracy: 82.8813\n",
            "\n",
            "Time taken for the epoch 15.0846\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▄▇▇█████</td></tr><tr><td>train loss</td><td>█▆▄▂▂▁▁▁▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▁▂▅▆▇▇████</td></tr><tr><td>val loss</td><td>██▄▄▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>95.37332</td></tr><tr><td>train loss</td><td>0.1385</td></tr><tr><td>training time</td><td>15.08459</td></tr><tr><td>val acc</td><td>82.88126</td></tr><tr><td>val loss</td><td>1.12644</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">mild-sweep-1</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/xr2bqdyn' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/xr2bqdyn</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_162414-xr2bqdyn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 95pskfmj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_162714-95pskfmj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/95pskfmj' target=\"_blank\">eager-sweep-2</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/95pskfmj' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/95pskfmj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9832\n",
            "Batch 100 Loss 1.0314\n",
            "Batch 200 Loss 0.9649\n",
            "Batch 300 Loss 0.8532\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.0457 Train Accuracy: 68.5121 Validation Loss: 2.5884 Validation Accuracy: 46.2608\n",
            "\n",
            "Time taken for the epoch 40.4929\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8951\n",
            "Batch 100 Loss 0.8375\n",
            "Batch 200 Loss 0.7163\n",
            "Batch 300 Loss 0.7093\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7802 Train Accuracy: 75.7276 Validation Loss: 2.3865 Validation Accuracy: 53.4320\n",
            "\n",
            "Time taken for the epoch 21.8126\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6199\n",
            "Batch 100 Loss 0.5598\n",
            "Batch 200 Loss 0.5255\n",
            "Batch 300 Loss 0.4494\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5256 Train Accuracy: 82.0221 Validation Loss: 1.7944 Validation Accuracy: 65.4901\n",
            "\n",
            "Time taken for the epoch 21.4828\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3943\n",
            "Batch 100 Loss 0.3378\n",
            "Batch 200 Loss 0.3435\n",
            "Batch 300 Loss 0.3116\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3414 Train Accuracy: 88.1460 Validation Loss: 1.4373 Validation Accuracy: 74.4265\n",
            "\n",
            "Time taken for the epoch 21.4936\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2838\n",
            "Batch 100 Loss 0.2974\n",
            "Batch 200 Loss 0.2711\n",
            "Batch 300 Loss 0.2528\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2472 Train Accuracy: 91.4776 Validation Loss: 1.3509 Validation Accuracy: 77.5666\n",
            "\n",
            "Time taken for the epoch 22.2715\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2141\n",
            "Batch 100 Loss 0.2044\n",
            "Batch 200 Loss 0.1995\n",
            "Batch 300 Loss 0.1866\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1917 Train Accuracy: 93.5193 Validation Loss: 1.2350 Validation Accuracy: 80.5612\n",
            "\n",
            "Time taken for the epoch 21.6125\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1716\n",
            "Batch 100 Loss 0.1710\n",
            "Batch 200 Loss 0.1994\n",
            "Batch 300 Loss 0.1629\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1590 Train Accuracy: 94.6761 Validation Loss: 1.2262 Validation Accuracy: 80.8289\n",
            "\n",
            "Time taken for the epoch 21.6286\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1627\n",
            "Batch 100 Loss 0.1372\n",
            "Batch 200 Loss 0.1449\n",
            "Batch 300 Loss 0.1231\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1374 Train Accuracy: 95.3943 Validation Loss: 1.2176 Validation Accuracy: 82.8462\n",
            "\n",
            "Time taken for the epoch 21.3609\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1296\n",
            "Batch 100 Loss 0.1271\n",
            "Batch 200 Loss 0.1113\n",
            "Batch 300 Loss 0.1188\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1209 Train Accuracy: 95.9742 Validation Loss: 1.1878 Validation Accuracy: 83.6348\n",
            "\n",
            "Time taken for the epoch 21.7602\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1359\n",
            "Batch 100 Loss 0.1100\n",
            "Batch 200 Loss 0.1039\n",
            "Batch 300 Loss 0.0929\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1083 Train Accuracy: 96.4808 Validation Loss: 1.2314 Validation Accuracy: 82.8525\n",
            "\n",
            "Time taken for the epoch 21.4994\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▄▆▇▇████</td></tr><tr><td>train loss</td><td>█▆▄▃▂▂▁▁▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▁▂▅▆▇▇▇███</td></tr><tr><td>val loss</td><td>█▇▄▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>96.48083</td></tr><tr><td>train loss</td><td>0.1083</td></tr><tr><td>training time</td><td>21.49943</td></tr><tr><td>val acc</td><td>82.85252</td></tr><tr><td>val loss</td><td>1.23143</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-sweep-2</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/95pskfmj' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/95pskfmj</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_162714-95pskfmj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jtuck3vw with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_163125-jtuck3vw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/jtuck3vw' target=\"_blank\">devoted-sweep-3</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/jtuck3vw' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/jtuck3vw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9902\n",
            "Batch 100 Loss 1.1465\n",
            "Batch 200 Loss 0.9604\n",
            "Batch 300 Loss 1.0043\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.2269 Train Accuracy: 65.8721 Validation Loss: 1.8301 Validation Accuracy: 58.3261\n",
            "\n",
            "Time taken for the epoch 45.3325\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9415\n",
            "Batch 100 Loss 0.9230\n",
            "Batch 200 Loss 0.9316\n",
            "Batch 300 Loss 0.8767\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9381 Train Accuracy: 72.7942 Validation Loss: 2.1948 Validation Accuracy: 52.5495\n",
            "\n",
            "Time taken for the epoch 18.6182\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8825\n",
            "Batch 100 Loss 0.9449\n",
            "Batch 200 Loss 0.8799\n",
            "Batch 300 Loss 0.8782\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8878 Train Accuracy: 74.0083 Validation Loss: 2.2061 Validation Accuracy: 53.5198\n",
            "\n",
            "Time taken for the epoch 20.0677\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8346\n",
            "Batch 100 Loss 0.8365\n",
            "Batch 200 Loss 0.8025\n",
            "Batch 300 Loss 0.7395\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8141 Train Accuracy: 75.6789 Validation Loss: 2.3078 Validation Accuracy: 53.9122\n",
            "\n",
            "Time taken for the epoch 19.6183\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7601\n",
            "Batch 100 Loss 0.7444\n",
            "Batch 200 Loss 0.6804\n",
            "Batch 300 Loss 0.6629\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7095 Train Accuracy: 78.1429 Validation Loss: 1.9428 Validation Accuracy: 61.2614\n",
            "\n",
            "Time taken for the epoch 20.4881\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6455\n",
            "Batch 100 Loss 0.5897\n",
            "Batch 200 Loss 0.5618\n",
            "Batch 300 Loss 0.4997\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5652 Train Accuracy: 82.0919 Validation Loss: 1.7771 Validation Accuracy: 66.7391\n",
            "\n",
            "Time taken for the epoch 18.6169\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.5136\n",
            "Batch 100 Loss 0.4154\n",
            "Batch 200 Loss 0.3944\n",
            "Batch 300 Loss 0.3832\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.4117 Train Accuracy: 86.5030 Validation Loss: 1.5354 Validation Accuracy: 74.7743\n",
            "\n",
            "Time taken for the epoch 20.3783\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3010\n",
            "Batch 100 Loss 0.2863\n",
            "Batch 200 Loss 0.3138\n",
            "Batch 300 Loss 0.3111\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2970 Train Accuracy: 90.7751 Validation Loss: 1.4613 Validation Accuracy: 77.7615\n",
            "\n",
            "Time taken for the epoch 19.5897\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2846\n",
            "Batch 100 Loss 0.2694\n",
            "Batch 200 Loss 0.2111\n",
            "Batch 300 Loss 0.2297\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2450 Train Accuracy: 92.3517 Validation Loss: 1.4383 Validation Accuracy: 78.8487\n",
            "\n",
            "Time taken for the epoch 19.2817\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2312\n",
            "Batch 100 Loss 0.2345\n",
            "Batch 200 Loss 0.2142\n",
            "Batch 300 Loss 0.1862\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2130 Train Accuracy: 93.3643 Validation Loss: 1.3855 Validation Accuracy: 80.7887\n",
            "\n",
            "Time taken for the epoch 20.8983\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▃▃▄▅▆▇██</td></tr><tr><td>train loss</td><td>█▆▆▅▄▃▂▂▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val acc</td><td>▂▁▁▁▃▅▇▇██</td></tr><tr><td>val loss</td><td>▄▇▇█▅▄▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>93.36434</td></tr><tr><td>train loss</td><td>0.21297</td></tr><tr><td>training time</td><td>20.89827</td></tr><tr><td>val acc</td><td>80.78865</td></tr><tr><td>val loss</td><td>1.38554</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devoted-sweep-3</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/jtuck3vw' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/jtuck3vw</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_163125-jtuck3vw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cfkb4mvl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_163517-cfkb4mvl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/cfkb4mvl' target=\"_blank\">misty-sweep-4</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/cfkb4mvl' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/cfkb4mvl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9901\n",
            "Batch 100 Loss 1.0446\n",
            "Batch 200 Loss 0.9648\n",
            "Batch 300 Loss 0.9677\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.1418 Train Accuracy: 66.9102 Validation Loss: 2.1940 Validation Accuracy: 56.8442\n",
            "\n",
            "Time taken for the epoch 53.3451\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9459\n",
            "Batch 100 Loss 0.9256\n",
            "Batch 200 Loss 0.9123\n",
            "Batch 300 Loss 0.8855\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9242 Train Accuracy: 72.9054 Validation Loss: 2.3170 Validation Accuracy: 52.5104\n",
            "\n",
            "Time taken for the epoch 27.5894\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8649\n",
            "Batch 100 Loss 0.8739\n",
            "Batch 200 Loss 0.8743\n",
            "Batch 300 Loss 0.8516\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8478 Train Accuracy: 74.8677 Validation Loss: 2.2169 Validation Accuracy: 55.7763\n",
            "\n",
            "Time taken for the epoch 27.4429\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7865\n",
            "Batch 100 Loss 0.7641\n",
            "Batch 200 Loss 0.7138\n",
            "Batch 300 Loss 0.6357\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7057 Train Accuracy: 77.7382 Validation Loss: 2.0869 Validation Accuracy: 60.7262\n",
            "\n",
            "Time taken for the epoch 27.4822\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6395\n",
            "Batch 100 Loss 0.5661\n",
            "Batch 200 Loss 0.4821\n",
            "Batch 300 Loss 0.4634\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5182 Train Accuracy: 82.6176 Validation Loss: 1.6567 Validation Accuracy: 71.3930\n",
            "\n",
            "Time taken for the epoch 27.7513\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.4208\n",
            "Batch 100 Loss 0.3027\n",
            "Batch 200 Loss 0.2891\n",
            "Batch 300 Loss 0.3181\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3327 Train Accuracy: 88.7811 Validation Loss: 1.4496 Validation Accuracy: 77.9434\n",
            "\n",
            "Time taken for the epoch 27.3439\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2737\n",
            "Batch 100 Loss 0.1987\n",
            "Batch 200 Loss 0.2922\n",
            "Batch 300 Loss 0.2202\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2415 Train Accuracy: 92.3807 Validation Loss: 1.3427 Validation Accuracy: 80.7384\n",
            "\n",
            "Time taken for the epoch 26.9851\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1934\n",
            "Batch 100 Loss 0.2201\n",
            "Batch 200 Loss 0.1837\n",
            "Batch 300 Loss 0.1888\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1982 Train Accuracy: 93.6928 Validation Loss: 1.3238 Validation Accuracy: 81.7153\n",
            "\n",
            "Time taken for the epoch 27.1805\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1992\n",
            "Batch 100 Loss 0.1639\n",
            "Batch 200 Loss 0.1482\n",
            "Batch 300 Loss 0.1740\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1754 Train Accuracy: 94.3274 Validation Loss: 1.3228 Validation Accuracy: 81.3842\n",
            "\n",
            "Time taken for the epoch 27.6486\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1724\n",
            "Batch 100 Loss 0.1789\n",
            "Batch 200 Loss 0.1449\n",
            "Batch 300 Loss 0.1857\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1584 Train Accuracy: 94.6966 Validation Loss: 1.2430 Validation Accuracy: 82.5546\n",
            "\n",
            "Time taken for the epoch 27.2747\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▃▄▅▇▇███</td></tr><tr><td>train loss</td><td>█▆▆▅▄▂▂▁▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▂▁▂▃▅▇████</td></tr><tr><td>val loss</td><td>▇█▇▇▄▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>94.69659</td></tr><tr><td>train loss</td><td>0.15845</td></tr><tr><td>training time</td><td>27.27465</td></tr><tr><td>val acc</td><td>82.55457</td></tr><tr><td>val loss</td><td>1.24296</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">misty-sweep-4</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/cfkb4mvl' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/cfkb4mvl</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_163517-cfkb4mvl/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6yx73284 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_164025-6yx73284</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/6yx73284' target=\"_blank\">sunny-sweep-5</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/6yx73284' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/6yx73284</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state', 'seed_generator_4/seed_generator_state', 'seed_generator_5/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9903\n",
            "Batch 100 Loss 1.2170\n",
            "Batch 200 Loss 1.0958\n",
            "Batch 300 Loss 1.0074\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.3213 Train Accuracy: 64.6841 Validation Loss: 2.1843 Validation Accuracy: 56.6602\n",
            "\n",
            "Time taken for the epoch 58.3336\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 1.0106\n",
            "Batch 100 Loss 0.9812\n",
            "Batch 200 Loss 1.0091\n",
            "Batch 300 Loss 0.9504\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9904 Train Accuracy: 71.5816 Validation Loss: 2.1062 Validation Accuracy: 58.3431\n",
            "\n",
            "Time taken for the epoch 24.7585\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9091\n",
            "Batch 100 Loss 0.9504\n",
            "Batch 200 Loss 0.9200\n",
            "Batch 300 Loss 0.9308\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9337 Train Accuracy: 72.6939 Validation Loss: 2.2690 Validation Accuracy: 52.2041\n",
            "\n",
            "Time taken for the epoch 25.2791\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9210\n",
            "Batch 100 Loss 0.8933\n",
            "Batch 200 Loss 0.9033\n",
            "Batch 300 Loss 0.8999\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8993 Train Accuracy: 73.8212 Validation Loss: 2.7053 Validation Accuracy: 47.7608\n",
            "\n",
            "Time taken for the epoch 23.8946\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8798\n",
            "Batch 100 Loss 0.8582\n",
            "Batch 200 Loss 0.8335\n",
            "Batch 300 Loss 0.7706\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8416 Train Accuracy: 75.0933 Validation Loss: 2.3044 Validation Accuracy: 54.6494\n",
            "\n",
            "Time taken for the epoch 24.0134\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7924\n",
            "Batch 100 Loss 0.7513\n",
            "Batch 200 Loss 0.7648\n",
            "Batch 300 Loss 0.7278\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7796 Train Accuracy: 76.7896 Validation Loss: 2.4714 Validation Accuracy: 54.5932\n",
            "\n",
            "Time taken for the epoch 25.5257\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7612\n",
            "Batch 100 Loss 0.7143\n",
            "Batch 200 Loss 0.6790\n",
            "Batch 300 Loss 0.7141\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7196 Train Accuracy: 78.2400 Validation Loss: 2.5468 Validation Accuracy: 55.6287\n",
            "\n",
            "Time taken for the epoch 24.8900\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6808\n",
            "Batch 100 Loss 0.6590\n",
            "Batch 200 Loss 0.6171\n",
            "Batch 300 Loss 0.5974\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.6504 Train Accuracy: 79.7843 Validation Loss: 2.2299 Validation Accuracy: 60.3215\n",
            "\n",
            "Time taken for the epoch 25.2589\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6296\n",
            "Batch 100 Loss 0.6007\n",
            "Batch 200 Loss 0.5784\n",
            "Batch 300 Loss 0.5313\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5814 Train Accuracy: 81.8868 Validation Loss: 2.1011 Validation Accuracy: 63.7263\n",
            "\n",
            "Time taken for the epoch 24.6615\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.5697\n",
            "Batch 100 Loss 0.5062\n",
            "Batch 200 Loss 0.4908\n",
            "Batch 300 Loss 0.4616\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5007 Train Accuracy: 84.0036 Validation Loss: 2.0453 Validation Accuracy: 66.0693\n",
            "\n",
            "Time taken for the epoch 24.5491\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▄▄▅▅▆▆▇█</td></tr><tr><td>train loss</td><td>█▅▅▄▄▃▃▂▂▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▄▅▃▁▄▄▄▆▇█</td></tr><tr><td>val loss</td><td>▂▂▃█▄▆▆▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>84.0036</td></tr><tr><td>train loss</td><td>0.5007</td></tr><tr><td>training time</td><td>24.54906</td></tr><tr><td>val acc</td><td>66.06927</td></tr><tr><td>val loss</td><td>2.04526</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sunny-sweep-5</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/6yx73284' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/6yx73284</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_164025-6yx73284/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bt1bw46f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_164513-bt1bw46f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/bt1bw46f' target=\"_blank\">efficient-sweep-6</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/bt1bw46f' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/bt1bw46f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state', 'seed_generator_4/seed_generator_state', 'seed_generator_5/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9901\n",
            "Batch 100 Loss 1.1922\n",
            "Batch 200 Loss 0.9993\n",
            "Batch 300 Loss 0.9631\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.2210 Train Accuracy: 65.5381 Validation Loss: 2.6421 Validation Accuracy: 56.9297\n",
            "\n",
            "Time taken for the epoch 68.0251\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9742\n",
            "Batch 100 Loss 0.9369\n",
            "Batch 200 Loss 0.9894\n",
            "Batch 300 Loss 0.9513\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9746 Train Accuracy: 72.0945 Validation Loss: 2.4507 Validation Accuracy: 56.2173\n",
            "\n",
            "Time taken for the epoch 33.2382\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9513\n",
            "Batch 100 Loss 0.9427\n",
            "Batch 200 Loss 0.9317\n",
            "Batch 300 Loss 0.9362\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9551 Train Accuracy: 72.6987 Validation Loss: 2.7221 Validation Accuracy: 48.6244\n",
            "\n",
            "Time taken for the epoch 33.6260\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9663\n",
            "Batch 100 Loss 0.9023\n",
            "Batch 200 Loss 0.9646\n",
            "Batch 300 Loss 0.9564\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9452 Train Accuracy: 72.8459 Validation Loss: 3.3278 Validation Accuracy: 42.9545\n",
            "\n",
            "Time taken for the epoch 32.7756\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8912\n",
            "Batch 100 Loss 0.8798\n",
            "Batch 200 Loss 0.9381\n",
            "Batch 300 Loss 0.9347\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9352 Train Accuracy: 73.0252 Validation Loss: 2.5668 Validation Accuracy: 56.0450\n",
            "\n",
            "Time taken for the epoch 33.5132\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9116\n",
            "Batch 100 Loss 0.9033\n",
            "Batch 200 Loss 0.9069\n",
            "Batch 300 Loss 0.8782\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8999 Train Accuracy: 73.4825 Validation Loss: 3.3510 Validation Accuracy: 44.9208\n",
            "\n",
            "Time taken for the epoch 33.3673\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8492\n",
            "Batch 100 Loss 0.8092\n",
            "Batch 200 Loss 0.7937\n",
            "Batch 300 Loss 0.7102\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7843 Train Accuracy: 76.0680 Validation Loss: 2.5303 Validation Accuracy: 55.0068\n",
            "\n",
            "Time taken for the epoch 33.6300\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7255\n",
            "Batch 100 Loss 0.7019\n",
            "Batch 200 Loss 0.6897\n",
            "Batch 300 Loss 0.5973\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.6820 Train Accuracy: 79.1300 Validation Loss: 2.3846 Validation Accuracy: 59.5811\n",
            "\n",
            "Time taken for the epoch 33.7870\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6399\n",
            "Batch 100 Loss 0.5882\n",
            "Batch 200 Loss 0.5701\n",
            "Batch 300 Loss 0.5471\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5752 Train Accuracy: 81.8141 Validation Loss: 2.3159 Validation Accuracy: 62.6173\n",
            "\n",
            "Time taken for the epoch 33.1061\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.5187\n",
            "Batch 100 Loss 0.5291\n",
            "Batch 200 Loss 0.4222\n",
            "Batch 300 Loss 0.4159\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.4399 Train Accuracy: 85.2659 Validation Loss: 1.7855 Validation Accuracy: 71.5818\n",
            "\n",
            "Time taken for the epoch 33.8807\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▄▄▄▄▅▆▇█</td></tr><tr><td>train loss</td><td>█▆▆▆▅▅▄▃▂▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▄▄▂▁▄▁▄▅▆█</td></tr><tr><td>val loss</td><td>▅▄▅█▄█▄▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>85.26593</td></tr><tr><td>train loss</td><td>0.43986</td></tr><tr><td>training time</td><td>33.88072</td></tr><tr><td>val acc</td><td>71.58176</td></tr><tr><td>val loss</td><td>1.7855</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">efficient-sweep-6</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/bt1bw46f' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/bt1bw46f</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_164513-bt1bw46f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fhueo72d with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_165133-fhueo72d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/fhueo72d' target=\"_blank\">logical-sweep-7</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/fhueo72d' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/fhueo72d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9850\n",
            "Batch 100 Loss 1.0654\n",
            "Batch 200 Loss 1.0271\n",
            "Batch 300 Loss 0.9410\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.0939 Train Accuracy: 68.0157 Validation Loss: 2.2856 Validation Accuracy: 48.6540\n",
            "\n",
            "Time taken for the epoch 38.6951\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9148\n",
            "Batch 100 Loss 0.8593\n",
            "Batch 200 Loss 0.8590\n",
            "Batch 300 Loss 0.7603\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8347 Train Accuracy: 74.8832 Validation Loss: 2.0361 Validation Accuracy: 55.1882\n",
            "\n",
            "Time taken for the epoch 17.3255\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7468\n",
            "Batch 100 Loss 0.6327\n",
            "Batch 200 Loss 0.5139\n",
            "Batch 300 Loss 0.4593\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5503 Train Accuracy: 80.8551 Validation Loss: 1.3813 Validation Accuracy: 72.4558\n",
            "\n",
            "Time taken for the epoch 15.9940\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3546\n",
            "Batch 100 Loss 0.3524\n",
            "Batch 200 Loss 0.3209\n",
            "Batch 300 Loss 0.2991\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3121 Train Accuracy: 89.3999 Validation Loss: 1.3165 Validation Accuracy: 76.1836\n",
            "\n",
            "Time taken for the epoch 16.0522\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2762\n",
            "Batch 100 Loss 0.2665\n",
            "Batch 200 Loss 0.2461\n",
            "Batch 300 Loss 0.2125\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2446 Train Accuracy: 91.9491 Validation Loss: 1.2331 Validation Accuracy: 78.8660\n",
            "\n",
            "Time taken for the epoch 15.4200\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2377\n",
            "Batch 100 Loss 0.2159\n",
            "Batch 200 Loss 0.2174\n",
            "Batch 300 Loss 0.2450\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2130 Train Accuracy: 92.9472 Validation Loss: 1.2734 Validation Accuracy: 79.2240\n",
            "\n",
            "Time taken for the epoch 15.9665\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2254\n",
            "Batch 100 Loss 0.1975\n",
            "Batch 200 Loss 0.2018\n",
            "Batch 300 Loss 0.1906\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1947 Train Accuracy: 93.4774 Validation Loss: 1.2238 Validation Accuracy: 80.6168\n",
            "\n",
            "Time taken for the epoch 16.3957\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1938\n",
            "Batch 100 Loss 0.1770\n",
            "Batch 200 Loss 0.1864\n",
            "Batch 300 Loss 0.1662\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1799 Train Accuracy: 93.9446 Validation Loss: 1.2122 Validation Accuracy: 80.7237\n",
            "\n",
            "Time taken for the epoch 17.3685\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1855\n",
            "Batch 100 Loss 0.1382\n",
            "Batch 200 Loss 0.1764\n",
            "Batch 300 Loss 0.1620\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1695 Train Accuracy: 94.3335 Validation Loss: 1.2047 Validation Accuracy: 81.6798\n",
            "\n",
            "Time taken for the epoch 16.4487\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1512\n",
            "Batch 100 Loss 0.1575\n",
            "Batch 200 Loss 0.1322\n",
            "Batch 300 Loss 0.1725\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1608 Train Accuracy: 94.5651 Validation Loss: 1.2155 Validation Accuracy: 81.1103\n",
            "\n",
            "Time taken for the epoch 16.1086\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▄▇▇█████</td></tr><tr><td>train loss</td><td>█▆▄▂▂▁▁▁▁▁</td></tr><tr><td>training time</td><td>█▂▁▁▁▁▁▂▁▁</td></tr><tr><td>val acc</td><td>▁▂▆▇▇▇████</td></tr><tr><td>val loss</td><td>█▆▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>94.56509</td></tr><tr><td>train loss</td><td>0.16079</td></tr><tr><td>training time</td><td>16.10859</td></tr><tr><td>val acc</td><td>81.11031</td></tr><tr><td>val loss</td><td>1.21546</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">logical-sweep-7</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/fhueo72d' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/fhueo72d</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_165133-fhueo72d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ddgaz3v with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_165449-7ddgaz3v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/7ddgaz3v' target=\"_blank\">pious-sweep-8</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/7ddgaz3v' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/7ddgaz3v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9862\n",
            "Batch 100 Loss 1.0537\n",
            "Batch 200 Loss 0.9540\n",
            "Batch 300 Loss 0.9048\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.0543 Train Accuracy: 68.5014 Validation Loss: 2.8224 Validation Accuracy: 44.5469\n",
            "\n",
            "Time taken for the epoch 42.6124\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9495\n",
            "Batch 100 Loss 0.8628\n",
            "Batch 200 Loss 0.7938\n",
            "Batch 300 Loss 0.7595\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8065 Train Accuracy: 75.4733 Validation Loss: 2.1360 Validation Accuracy: 55.7256\n",
            "\n",
            "Time taken for the epoch 22.5422\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6803\n",
            "Batch 100 Loss 0.6377\n",
            "Batch 200 Loss 0.5980\n",
            "Batch 300 Loss 0.5827\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.6173 Train Accuracy: 79.8480 Validation Loss: 2.0623 Validation Accuracy: 60.7089\n",
            "\n",
            "Time taken for the epoch 22.1450\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.5641\n",
            "Batch 100 Loss 0.4779\n",
            "Batch 200 Loss 0.4197\n",
            "Batch 300 Loss 0.3531\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.4491 Train Accuracy: 84.5356 Validation Loss: 1.7602 Validation Accuracy: 67.8013\n",
            "\n",
            "Time taken for the epoch 22.5600\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3643\n",
            "Batch 100 Loss 0.3319\n",
            "Batch 200 Loss 0.2882\n",
            "Batch 300 Loss 0.2688\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3106 Train Accuracy: 88.9420 Validation Loss: 1.3821 Validation Accuracy: 75.6179\n",
            "\n",
            "Time taken for the epoch 22.1962\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2499\n",
            "Batch 100 Loss 0.2317\n",
            "Batch 200 Loss 0.2279\n",
            "Batch 300 Loss 0.2117\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2257 Train Accuracy: 92.2175 Validation Loss: 1.2686 Validation Accuracy: 79.2665\n",
            "\n",
            "Time taken for the epoch 22.4520\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1924\n",
            "Batch 100 Loss 0.2133\n",
            "Batch 200 Loss 0.1777\n",
            "Batch 300 Loss 0.1776\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1849 Train Accuracy: 93.7446 Validation Loss: 1.2658 Validation Accuracy: 80.4881\n",
            "\n",
            "Time taken for the epoch 22.5224\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1589\n",
            "Batch 100 Loss 0.1461\n",
            "Batch 200 Loss 0.1605\n",
            "Batch 300 Loss 0.1772\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1600 Train Accuracy: 94.6278 Validation Loss: 1.2468 Validation Accuracy: 81.5503\n",
            "\n",
            "Time taken for the epoch 22.0470\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1546\n",
            "Batch 100 Loss 0.1689\n",
            "Batch 200 Loss 0.1355\n",
            "Batch 300 Loss 0.1653\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1429 Train Accuracy: 95.2006 Validation Loss: 1.2260 Validation Accuracy: 82.0600\n",
            "\n",
            "Time taken for the epoch 22.3709\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1152\n",
            "Batch 100 Loss 0.1221\n",
            "Batch 200 Loss 0.1421\n",
            "Batch 300 Loss 0.1312\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1297 Train Accuracy: 95.6731 Validation Loss: 1.2272 Validation Accuracy: 82.8726\n",
            "\n",
            "Time taken for the epoch 22.4009\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▄▅▆▇████</td></tr><tr><td>train loss</td><td>█▆▅▃▂▂▁▁▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▁▃▄▅▇▇████</td></tr><tr><td>val loss</td><td>█▅▅▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>95.67312</td></tr><tr><td>train loss</td><td>0.12968</td></tr><tr><td>training time</td><td>22.40085</td></tr><tr><td>val acc</td><td>82.87256</td></tr><tr><td>val loss</td><td>1.22717</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pious-sweep-8</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/7ddgaz3v' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/7ddgaz3v</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_165449-7ddgaz3v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sdofc0f4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_165902-sdofc0f4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/sdofc0f4' target=\"_blank\">true-sweep-9</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/sdofc0f4' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/sdofc0f4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9904\n",
            "Batch 100 Loss 1.0696\n",
            "Batch 200 Loss 1.0302\n",
            "Batch 300 Loss 1.0347\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.2400 Train Accuracy: 65.8304 Validation Loss: 2.3416 Validation Accuracy: 56.9141\n",
            "\n",
            "Time taken for the epoch 52.6595\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9576\n",
            "Batch 100 Loss 0.9599\n",
            "Batch 200 Loss 0.9737\n",
            "Batch 300 Loss 0.9131\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9725 Train Accuracy: 71.8226 Validation Loss: 2.2339 Validation Accuracy: 53.3786\n",
            "\n",
            "Time taken for the epoch 22.4677\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9093\n",
            "Batch 100 Loss 0.9447\n",
            "Batch 200 Loss 0.9085\n",
            "Batch 300 Loss 0.9280\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9175 Train Accuracy: 73.6079 Validation Loss: 2.3295 Validation Accuracy: 52.7876\n",
            "\n",
            "Time taken for the epoch 21.8022\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9506\n",
            "Batch 100 Loss 0.8864\n",
            "Batch 200 Loss 0.8689\n",
            "Batch 300 Loss 0.8207\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8610 Train Accuracy: 74.6601 Validation Loss: 2.2397 Validation Accuracy: 55.1261\n",
            "\n",
            "Time taken for the epoch 23.6331\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8289\n",
            "Batch 100 Loss 0.7517\n",
            "Batch 200 Loss 0.7903\n",
            "Batch 300 Loss 0.7858\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7873 Train Accuracy: 76.2992 Validation Loss: 2.2741 Validation Accuracy: 54.5890\n",
            "\n",
            "Time taken for the epoch 22.4580\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7471\n",
            "Batch 100 Loss 0.7009\n",
            "Batch 200 Loss 0.7041\n",
            "Batch 300 Loss 0.6459\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.6989 Train Accuracy: 78.3170 Validation Loss: 1.9999 Validation Accuracy: 60.1425\n",
            "\n",
            "Time taken for the epoch 22.9872\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6154\n",
            "Batch 100 Loss 0.6030\n",
            "Batch 200 Loss 0.5960\n",
            "Batch 300 Loss 0.5635\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5881 Train Accuracy: 81.2061 Validation Loss: 1.7748 Validation Accuracy: 65.6627\n",
            "\n",
            "Time taken for the epoch 23.2925\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.5519\n",
            "Batch 100 Loss 0.5279\n",
            "Batch 200 Loss 0.4198\n",
            "Batch 300 Loss 0.5108\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.4723 Train Accuracy: 84.6209 Validation Loss: 1.6670 Validation Accuracy: 69.9811\n",
            "\n",
            "Time taken for the epoch 22.2370\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.4005\n",
            "Batch 100 Loss 0.3755\n",
            "Batch 200 Loss 0.4010\n",
            "Batch 300 Loss 0.3691\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3787 Train Accuracy: 87.5698 Validation Loss: 1.5046 Validation Accuracy: 74.6373\n",
            "\n",
            "Time taken for the epoch 23.0812\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3271\n",
            "Batch 100 Loss 0.3334\n",
            "Batch 200 Loss 0.3093\n",
            "Batch 300 Loss 0.3377\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3150 Train Accuracy: 89.6883 Validation Loss: 1.4358 Validation Accuracy: 77.0904\n",
            "\n",
            "Time taken for the epoch 23.5157\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▃▄▄▅▆▇▇█</td></tr><tr><td>train loss</td><td>█▆▆▅▅▄▃▂▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▂▁▁▂▂▃▅▆▇█</td></tr><tr><td>val loss</td><td>█▇█▇▇▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>89.68827</td></tr><tr><td>train loss</td><td>0.315</td></tr><tr><td>training time</td><td>23.51565</td></tr><tr><td>val acc</td><td>77.09041</td></tr><tr><td>val loss</td><td>1.43578</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">true-sweep-9</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/sdofc0f4' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/sdofc0f4</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_165902-sdofc0f4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1ko1dgnu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_170328-1ko1dgnu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/1ko1dgnu' target=\"_blank\">brisk-sweep-10</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/1ko1dgnu' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/1ko1dgnu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9896\n",
            "Batch 100 Loss 1.1167\n",
            "Batch 200 Loss 1.0151\n",
            "Batch 300 Loss 1.0402\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.1523 Train Accuracy: 67.1780 Validation Loss: 2.4376 Validation Accuracy: 56.1876\n",
            "\n",
            "Time taken for the epoch 59.1593\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 1.0543\n",
            "Batch 100 Loss 1.0025\n",
            "Batch 200 Loss 0.9007\n",
            "Batch 300 Loss 0.9683\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9391 Train Accuracy: 72.6204 Validation Loss: 2.3409 Validation Accuracy: 52.7784\n",
            "\n",
            "Time taken for the epoch 28.8436\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9293\n",
            "Batch 100 Loss 0.8513\n",
            "Batch 200 Loss 0.8638\n",
            "Batch 300 Loss 0.8179\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8603 Train Accuracy: 74.3912 Validation Loss: 2.0955 Validation Accuracy: 57.3953\n",
            "\n",
            "Time taken for the epoch 41.8518\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8397\n",
            "Batch 100 Loss 0.7352\n",
            "Batch 200 Loss 0.7453\n",
            "Batch 300 Loss 0.7118\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7316 Train Accuracy: 76.8642 Validation Loss: 1.8916 Validation Accuracy: 62.9542\n",
            "\n",
            "Time taken for the epoch 29.1454\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7066\n",
            "Batch 100 Loss 0.6159\n",
            "Batch 200 Loss 0.5584\n",
            "Batch 300 Loss 0.5414\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5730 Train Accuracy: 80.7870 Validation Loss: 1.7423 Validation Accuracy: 68.9142\n",
            "\n",
            "Time taken for the epoch 29.0660\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.4916\n",
            "Batch 100 Loss 0.4353\n",
            "Batch 200 Loss 0.3656\n",
            "Batch 300 Loss 0.3840\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.4126 Train Accuracy: 85.9894 Validation Loss: 1.5044 Validation Accuracy: 74.3905\n",
            "\n",
            "Time taken for the epoch 28.8463\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3490\n",
            "Batch 100 Loss 0.3185\n",
            "Batch 200 Loss 0.3168\n",
            "Batch 300 Loss 0.2515\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3096 Train Accuracy: 89.6228 Validation Loss: 1.4151 Validation Accuracy: 77.8444\n",
            "\n",
            "Time taken for the epoch 29.6568\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2776\n",
            "Batch 100 Loss 0.2699\n",
            "Batch 200 Loss 0.2440\n",
            "Batch 300 Loss 0.2148\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2533 Train Accuracy: 91.5900 Validation Loss: 1.3698 Validation Accuracy: 78.3364\n",
            "\n",
            "Time taken for the epoch 30.2350\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2184\n",
            "Batch 100 Loss 0.2535\n",
            "Batch 200 Loss 0.2270\n",
            "Batch 300 Loss 0.2114\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2185 Train Accuracy: 92.7608 Validation Loss: 1.3544 Validation Accuracy: 79.7486\n",
            "\n",
            "Time taken for the epoch 28.8596\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1971\n",
            "Batch 100 Loss 0.2049\n",
            "Batch 200 Loss 0.1846\n",
            "Batch 300 Loss 0.1786\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1950 Train Accuracy: 93.4633 Validation Loss: 1.3623 Validation Accuracy: 80.4215\n",
            "\n",
            "Time taken for the epoch 29.6310\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train loss</td><td>█▆▆▅▄▃▂▁▁▁</td></tr><tr><td>training time</td><td>█▁▄▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▂▁▂▄▅▆▇▇██</td></tr><tr><td>val loss</td><td>█▇▆▄▄▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>93.46329</td></tr><tr><td>train loss</td><td>0.19496</td></tr><tr><td>training time</td><td>29.63102</td></tr><tr><td>val acc</td><td>80.42148</td></tr><tr><td>val loss</td><td>1.36227</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">brisk-sweep-10</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/1ko1dgnu' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/1ko1dgnu</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_170328-1ko1dgnu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: obmtxlku with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_170912-obmtxlku</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/obmtxlku' target=\"_blank\">twilight-sweep-11</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/obmtxlku' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/obmtxlku</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state', 'seed_generator_4/seed_generator_state', 'seed_generator_5/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9901\n",
            "Batch 100 Loss 1.2571\n",
            "Batch 200 Loss 1.1683\n",
            "Batch 300 Loss 1.0976\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.3543 Train Accuracy: 64.0846 Validation Loss: 2.4348 Validation Accuracy: 56.4790\n",
            "\n",
            "Time taken for the epoch 73.7232\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 1.0840\n",
            "Batch 100 Loss 0.9880\n",
            "Batch 200 Loss 0.9794\n",
            "Batch 300 Loss 1.0261\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.0050 Train Accuracy: 71.3672 Validation Loss: 2.5526 Validation Accuracy: 56.7310\n",
            "\n",
            "Time taken for the epoch 30.7541\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9742\n",
            "Batch 100 Loss 0.9861\n",
            "Batch 200 Loss 0.8606\n",
            "Batch 300 Loss 0.8402\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9337 Train Accuracy: 72.6450 Validation Loss: 2.0758 Validation Accuracy: 57.2484\n",
            "\n",
            "Time taken for the epoch 31.5356\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8840\n",
            "Batch 100 Loss 0.8767\n",
            "Batch 200 Loss 0.8343\n",
            "Batch 300 Loss 0.8650\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8652 Train Accuracy: 74.7178 Validation Loss: 2.3743 Validation Accuracy: 52.4441\n",
            "\n",
            "Time taken for the epoch 33.0932\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8230\n",
            "Batch 100 Loss 0.7996\n",
            "Batch 200 Loss 0.7984\n",
            "Batch 300 Loss 0.8196\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8311 Train Accuracy: 75.5777 Validation Loss: 2.5599 Validation Accuracy: 50.3755\n",
            "\n",
            "Time taken for the epoch 30.9128\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7882\n",
            "Batch 100 Loss 0.8067\n",
            "Batch 200 Loss 0.7959\n",
            "Batch 300 Loss 0.8482\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8045 Train Accuracy: 76.3869 Validation Loss: 2.4462 Validation Accuracy: 52.6617\n",
            "\n",
            "Time taken for the epoch 31.2141\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8418\n",
            "Batch 100 Loss 0.8058\n",
            "Batch 200 Loss 0.7576\n",
            "Batch 300 Loss 0.7980\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7843 Train Accuracy: 76.7772 Validation Loss: 2.2149 Validation Accuracy: 56.4395\n",
            "\n",
            "Time taken for the epoch 30.5223\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8010\n",
            "Batch 100 Loss 0.7620\n",
            "Batch 200 Loss 0.7643\n",
            "Batch 300 Loss 0.7806\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7650 Train Accuracy: 77.3563 Validation Loss: 2.1371 Validation Accuracy: 58.2322\n",
            "\n",
            "Time taken for the epoch 31.4649\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7952\n",
            "Batch 100 Loss 0.6923\n",
            "Batch 200 Loss 0.7108\n",
            "Batch 300 Loss 0.7013\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7454 Train Accuracy: 77.8702 Validation Loss: 2.1689 Validation Accuracy: 57.8800\n",
            "\n",
            "Time taken for the epoch 31.2033\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7411\n",
            "Batch 100 Loss 0.6976\n",
            "Batch 200 Loss 0.7554\n",
            "Batch 300 Loss 0.7256\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7238 Train Accuracy: 78.2723 Validation Loss: 2.3948 Validation Accuracy: 55.1188\n",
            "\n",
            "Time taken for the epoch 31.6614\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▅▅▆▇▇▇███</td></tr><tr><td>train loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▆▇▇▃▁▃▆██▅</td></tr><tr><td>val loss</td><td>▆█▁▅█▆▃▂▂▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>78.27227</td></tr><tr><td>train loss</td><td>0.72384</td></tr><tr><td>training time</td><td>31.66145</td></tr><tr><td>val acc</td><td>55.1188</td></tr><tr><td>val loss</td><td>2.3948</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">twilight-sweep-11</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/obmtxlku' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/obmtxlku</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_170912-obmtxlku/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 310k3uat with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Assignment_3' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_171517-310k3uat</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/310k3uat' target=\"_blank\">super-sweep-12</a></strong> to <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/sweeps/691yfjl7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/310k3uat' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/310k3uat</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['seed_generator/seed_generator_state', 'seed_generator_1/seed_generator_state', 'seed_generator_2/seed_generator_state', 'seed_generator_3/seed_generator_state', 'seed_generator_4/seed_generator_state', 'seed_generator_5/seed_generator_state'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss 3.9901\n",
            "Batch 100 Loss 1.2173\n",
            "Batch 200 Loss 1.0161\n",
            "Batch 300 Loss 1.0019\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.2404 Train Accuracy: 65.1121 Validation Loss: 2.5395 Validation Accuracy: 56.5216\n",
            "\n",
            "Time taken for the epoch 76.5159\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 1.0099\n",
            "Batch 100 Loss 1.0116\n",
            "Batch 200 Loss 0.9842\n",
            "Batch 300 Loss 1.0069\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9870 Train Accuracy: 71.5982 Validation Loss: 2.5902 Validation Accuracy: 53.9749\n",
            "\n",
            "Time taken for the epoch 36.4652\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.9605\n",
            "Batch 100 Loss 1.0289\n",
            "Batch 200 Loss 0.9536\n",
            "Batch 300 Loss 0.9131\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9669 Train Accuracy: 72.3644 Validation Loss: 2.8053 Validation Accuracy: 48.9030\n",
            "\n",
            "Time taken for the epoch 37.0005\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 1.0002\n",
            "Batch 100 Loss 0.9705\n",
            "Batch 200 Loss 0.9445\n",
            "Batch 300 Loss 0.9407\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.9466 Train Accuracy: 72.6559 Validation Loss: 2.2632 Validation Accuracy: 56.7113\n",
            "\n",
            "Time taken for the epoch 36.6618\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8870\n",
            "Batch 100 Loss 0.8829\n",
            "Batch 200 Loss 0.8416\n",
            "Batch 300 Loss 0.7599\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.8324 Train Accuracy: 75.0450 Validation Loss: 2.3111 Validation Accuracy: 56.8326\n",
            "\n",
            "Time taken for the epoch 36.0205\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.7916\n",
            "Batch 100 Loss 0.7392\n",
            "Batch 200 Loss 0.7284\n",
            "Batch 300 Loss 0.7035\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7189 Train Accuracy: 78.1650 Validation Loss: 2.4099 Validation Accuracy: 58.2550\n",
            "\n",
            "Time taken for the epoch 37.3210\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6491\n",
            "Batch 100 Loss 0.6525\n",
            "Batch 200 Loss 0.6990\n",
            "Batch 300 Loss 0.5507\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.6310 Train Accuracy: 80.9235 Validation Loss: 2.1370 Validation Accuracy: 62.5028\n",
            "\n",
            "Time taken for the epoch 37.1067\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.5352\n",
            "Batch 100 Loss 0.5446\n",
            "Batch 200 Loss 0.5228\n",
            "Batch 300 Loss 0.4602\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5131 Train Accuracy: 83.4917 Validation Loss: 1.8677 Validation Accuracy: 68.5354\n",
            "\n",
            "Time taken for the epoch 36.4625\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.4537\n",
            "Batch 100 Loss 0.3904\n",
            "Batch 200 Loss 0.3480\n",
            "Batch 300 Loss 0.3162\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3820 Train Accuracy: 87.0502 Validation Loss: 1.5796 Validation Accuracy: 74.8953\n",
            "\n",
            "Time taken for the epoch 37.1394\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.3372\n",
            "Batch 100 Loss 0.3009\n",
            "Batch 200 Loss 0.2609\n",
            "Batch 300 Loss 0.2735\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.2949 Train Accuracy: 90.0874 Validation Loss: 1.4936 Validation Accuracy: 77.0698\n",
            "\n",
            "Time taken for the epoch 35.4404\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train acc</td><td>▁▃▃▃▄▅▅▆▇█</td></tr><tr><td>train loss</td><td>█▆▆▆▅▄▃▃▂▁</td></tr><tr><td>training time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val acc</td><td>▃▂▁▃▃▃▄▆▇█</td></tr><tr><td>val loss</td><td>▇▇█▅▅▆▄▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train acc</td><td>90.08739</td></tr><tr><td>train loss</td><td>0.29485</td></tr><tr><td>training time</td><td>35.44043</td></tr><tr><td>val acc</td><td>77.06977</td></tr><tr><td>val loss</td><td>1.49357</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">super-sweep-12</strong> at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/310k3uat' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/310k3uat</a><br> View project at: <a href='https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/alokgaurav04-indian-institute-of-technology-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_171517-310k3uat/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ]
    }
  ]
}